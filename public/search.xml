<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[swoole_server_introduce]]></title>
    <url>%2F2019%2F05%2F27%2Fswoole-server-introduce%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 SwooleSwoole是面向生产环境的php异步网络通信引擎, php开发人员可以利用Swoole开发出高性能的server服务。Swoole的server部分, 内容很多, 也涉及很多的知识点, 本文仅对其server进行简单的概述, 具体的实现细节在后续的文章中再进行详细介绍。 1.2 网络编程 网络通信是指在一台(或者多台)机器上启动一个(或者多个)进程, 监听一个(或者多个)端口, 按照某种协议(可以是标准协议http, dns; 也可以是自行定义的协议)与客户端交换信息。 目前的网络编程多是在tcp, udp或者更上层的协议之上进行编程。Swoole的server部分是基于tcp以及udp协议的。 利用udp进行编程较为简单, 本文主要介绍tcp协议之上的网络编程 TCP网络编程主要涉及4种事件, 连接建立: 主要是指客户端发起连接(connect)以及服务端接受连接(accept) 消息到达: 服务端接受到客户端发送的数据, 该事件是TCP网络编程最重要的事件, 服务端对于该类事件进行处理时, 可以采用阻塞式或者非阻塞式, 除此之外, 服务端还需要考虑分包, 应用层缓冲区等问题 消息发送成功: 发送成功是指应用层将数据成功发送到内核的套接字发送缓冲区中, 并不是指客户端成功接受数据。对于低流量的服务而言, 数据通常一次性即可发送完, 并不需要关心此类事件。如果一次性不能将全部数据发送到内核缓冲区, 则需要关心消息是否成功发送(阻塞式编程在系统调用(write, writev, send等)返回后即是发送成功, 非阻塞式编程则需要考虑实际写入的数据是否与预期一致) 连接断开: 需要考虑客户端断开连接(read返回0)以及服务端断开连接(close, shutdown) tcp建立连接的过程如下图, 图中, ACK、SYN表示标志位, seq、ack为tcp包的序号以及确认序号 tcp断开连接的过程如下图, 上图考虑的是客户端主动断开连接的情况, 服务端主动断开连接也类似 图中, FIN、ACK表示标志位, seq、ack为tcp包的序号以及确认序号 1.3 进程间通信 进程之间的通信有无名管道(pipe), 有名管道(fifo), 信号(signal), 信号量(semaphore), 套接字(socket), 共享内存(shared memory)等方式 Swoole中采用unix域套接字(套接字的一种)用于多进程之间的通信(指Swoole内部进程之间) 1.4 socketpair socketpair用于创建一个套接字对, 类似于pipe, 不同的是pipe是单向通信, 双向通信需要创建两次, socketpair调用一次即可实现双向通信, 除此之外, 由于使用的是套接字, 还可以定义数据交换的方式 socketpair系统调用123456int socketpair(int domain, int type, int protocol, int sv[2]);//domain表示协议簇//type表示类型//protocol表示协议, SOCK_STREAM表示流协议(类似tcp), SOCK_DGRAM表示数据报协议(类似udp)//sv用于存储建立的套接字对, 也就是两个套接字文件描述符//成功返回0, 否则返回-1, 可以从errno获取错误信息 调用成功后sv[0], sv[1]分别存储一个文件描述符 向sv[0]中写入, 可以从sv[1]中读取 向sv[1]中写入, 可以从sv[0]中读取 进程调用socketpair后, fork子进程, 子进程会默认继承sv[0], sv[1]这两个文件描述符, 进而可以实现父子进程间通信。例如, 父进程向sv[0]中写入, 子进程从sv[1]中读取; 子进程向sv[1]中写入, 父进程从sv[0]中读取。 1.5 守护进程(daemon) 守护进程是一种特殊的后台进程, 它脱离于终端, 用于周期性的执行某种任务 进程组 每个进程都属于一个进程组 每个进程组都有一个进程组号, 也就是该组组长的进程号(PID) 一个进程只能为自己或者其子进程设置进程组号 会话 一个会话可以包含多个进程组, 这些进程组中最多只能有一个前台进程组(也可以没有), 其余为后台进程组 一个会话最多只能有一个控制终端 用户通过终端登录或者网络登录, 会创建一个新的会话 进程调用系统调用setsid可以创建一个新的会话, 调用setsid的进程不能是某个进程组的组长。setsid调用完成后, 该进程成为这个会话的首进程(领头进程), 同时变成一个新的进程组的组长, 如果该进程之前有控制终端, 该进程与终端的联系也被断开 创建守护进程的方式 fork子进程后, 父进程退出, 子进程执行setsid, 子进程即可成为守护进程。这种方式下, 子进程是会话的领头进程, 可以重新打开终端, 此时可以再次fork, fork产生的子进程无法再打开终端(只有会话的领头进程才能打开终端)。第二次fork并不是必须的, 只是为了防止子进程再次打开终端 linux提供了daemon函数(该函数并不是系统调用, 为库函数)用于创建守护进程 1.6 swoole tcp server示例1234567891011121314151617181920212223&lt;?php//创建server$serv = new Swoole\Server('0.0.0.0', 9501, SWOOLE_PROCESS, SWOOLE_SOCK_TCP);//设置server的参数$serv-&gt;set(array( 'reactor_num' =&gt; 2, //reactor thread num 'worker_num' =&gt; 3, //worker process num));//设置事件回调$serv-&gt;on('connect', function ($serv, $fd)&#123; echo "Client:Connect.\n";&#125;);$serv-&gt;on('receive', function ($serv, $fd, $reactor_id, $data) &#123; $serv-&gt;send($fd, 'Swoole: '.$data); $serv-&gt;close($fd);&#125;);$serv-&gt;on('close', function ($serv, $fd) &#123; echo "Client: Close.\n";&#125;);//启动server$serv-&gt;start(); 上述代码在cli模式下执行时, 经过词法分析, 语法分析生成opcode, 进而交由zend虚拟机执行 zend虚拟机在执行到$serv-&gt;start()时, 启动Swoole server 上述代码中设置的事件回调是在worker进程中执行, 后文会详细介绍Swoole server模型 二. swoole server2.1 base模式 说明 base模式采用多进程模型, 这种模型与nginx一致, 每个进程只有一个线程, 主进程负责管理工作进程, 工作进程负责监听端口, 接受连接, 处理请求以及关闭连接 多个进程同时监听端口, 会有惊群问题, linux 3.9之前版本的内核, Swoole没有解决惊群问题 linux 内核3.9及其后续版本提供了新的套接字参数SO_REUSEPORT, 该参数允许多个进程绑定到同一个端口, 内核在接受到新的连接请求时, 会唤醒其中一个进行处理, 内核层面也会做负载均衡, 可以解决上述的惊群问题, Swoole也已经加入了这个参数 base模式下, reactor_number参数并没有实际作用 如果worker进程数设置为1, 则不会fork出worker进程, 主进程直接处理请求, 这种模式适合调试 启动过程 php代码执行到$serv-&gt;start()时, 主进程进入int swServer_start(swServer *serv)函数, 该函数负责启动server 在函数swServer_start中会调用swReactorProcess_start, 这个函数会fork出多个worker进程 主进程和worker进程各自进入自己的事件循环, 处理各类事件 2.2 process模式 说明 这种模式为多进程多线程, 有主进程, manager进程, worker进程, task_worker进程 主进程下有多个线程, 主线程负责接受连接, 之后交给react线程处理请求。 react线程负责接收数据包, 并将数据转发给worker进程进行处理, 之后处理worker进程返回的数据 manager进程, 该进程为单线程, 主要负责管理worker进程, 类似于nginx中的主进程, 当worker进程异常退出时, manager进程负责重新fork出一个worker进程 worker进程, 该进程为单线程, 负责具体处理请求 task_worker进程, 用于处理比较耗时的任务, 默认不开启 worker进程与主进程中的react线程使用域套接字进行通信, worker进程之间不进行通信 启动过程 Swoole server启动入口: swServer_start函数, 1234567891011121314151617181920212223242526//php 代码中$serv-&gt;start(); 会调用函数, 进行server startint swServer_start(swServer *serv);// 该函数首先进行必要的参数检查static int swServer_start_check(swServer *serv);// 其中有,if (serv-&gt;worker_num &lt; serv-&gt;reactor_num)&#123; serv-&gt;reactor_num = serv-&gt;worker_num;&#125;//也就是说reactor_num &lt;= worker_num//之后执行factory start, 也就是swFactoryProcess_start函数, 该函数会fork出manager进程, manager进程进而fork出worker进程以及task_worker进程if (factory-&gt;start(factory) &lt; 0)&#123; return SW_ERR;&#125;//然后主进程的主线程生成reactor线程if (serv-&gt;factory_mode == SW_MODE_BASE)&#123; ret = swReactorProcess_start(serv);&#125;else&#123; ret = swReactorThread_start(serv);&#125; 如果设置了daemon模式, 在必要的参数检查完后, 先将自己变为守护进程再fork manager进程, 进而创建reactor线程 主进程先fork出manager进程, manager进程负责fork出worker进程以及task_worker进程。worker进程之后进入int swWorker_loop(swServer *serv, int worker_id), 也就是进入自己的事件循环, task_worker也是一样, 进入自己的事件循环。 1234static int swFactoryProcess_start(swFactory *factory);//swFactoryProcess_start会调用swManager_start生成manager进程int swManager_start(swServer *serv);// manager进程会fork出worker进程以及task_worker进程 主进程pthread_create出react线程, 主线程和react线程各自进入自己的事件循环, reactor线程执行static int swReactorThread_loop(swThreadParam *param), 等待处理事件 12//主线程执行swReactorThread_start, 创建出reactor线程int swReactorThread_start(swServer *serv); 结构图Swoole process模式结构如下图所示, 上图并没有考虑task_worker进程, 在默认情况下, task_worker进程数为0 三. 请求处理流程(process模式)3.1 reactor线程与worker进程之间的通信 Swoole master进程与worker进程之间的通信如下图所示, Swoole使用SOCK_DGRAM, 而不是SOCK_STREAM, 这里是因为每个reactor线程负责处理多个请求, reactor接收到请求后会将信息转发给worker进程, 由worker进程负责处理,如果使用SOCK_STREAM, worker进程无法对tcp进行分包, 进而处理请求 swFactoryProcess_start函数中会根据worker进程数创建对应个数的套接字对, 用于reactor线程与worker进程通信(详见swPipeUnsock_create函数) 假设reactor线程有2个, worker进程有3个, 则reactor与worker之间的通信如下图所示, 每个reactor线程负责监听几个worker进程, 每个worker进程只有一个reactor线程监听(reactor_num &lt;= worker_num)。Swoole默认使用worker_process_id % reactor_num对worker进程进行分配, 交给对应的reactor线程进行监听 reactor线程收到某个worker进程的数据后会进行处理, 值得注意的是, 这个reactor线程可能并不是发送请求的那个reactor线程。 reactor线程与worker进程通信的数据包123456789101112131415161718192021222324252627//包头typedef struct _swDataHead&#123; int fd; uint32_t len; int16_t from_id; uint8_t type; uint8_t flags; uint16_t from_fd;#ifdef SW_BUFFER_RECV_TIME double time;#endif&#125; swDataHead;//reactor线程向worker进程发送的数据, 也就是worker进程收到的数据包typedef struct&#123; swDataHead info; char data[SW_IPC_BUFFER_SIZE];&#125; swEventData;//worker进程向reactor线程发送的数据, 也就是reactor线程收到的数据包typedef struct&#123; swDataHead info; char data[0];&#125; swPipeBuffer; 3.2 请求处理 master进程中的主线程负责监听端口(listen), 接受连接(accept, 产生一个fd), 接受连接后将请求分配给reactor线程, 默认通过fd % reactor_number进行分配, 之后通过epoll_ctl将fd加入到对应reactor线程中, 刚加入时监听写事件, 因为新接受连接创建的套接字写缓冲区为空, 故而一定可写, 会被立刻触发, 进而reactor线程进行一些初始化操作 存在多个线程同时操作一个epollfd(通过系统调用epoll_create创建)的情况 多个线程同时调用epoll_ctl是线程安全的(对应一个epollfd), 一个线程正在执行, 其他线程会被阻塞(因为需要同时操作epoll底层的红黑树) 多个线程同时调用epoll_wait也是线程安全的, 但是一个事件可能会被多个线程同时接收到, 实际中不建议多个线程同时epoll_wait一个epollfd。Swoole中也是不存在这种情况的, Swoole中每个reactor线程都有自己的epollfd 一个线程调用epoll_wait, 一个线程调用epoll_ctl, 根据man手册, 如果epoll_ctl新加入的fd已经准备好, 会使得执行epoll_wait的线程变成非阻塞状态(可以通过man epoll_wait查看相关内容)。1234//主线程, 如果fd已经ready, reactor线程会被唤醒epoll_ctl(epollfd, fd, ...);//reactor线程epoll_wait(epollfd,...) reactor线程中fd的写事件被触发, reactor线程负责处理, 发现是首次加入, 没有数据可写, 则会开启读事件监听, 准备接受客户端发送的数据 reactor线程读取到用户的请求数据, 一个请求的数据接收完后, 将数据转发给worker进程, 默认是通过fd % worker_number进行分配 reactor发送给worker进程的数据包, 会包含一个头部, 头部中记录了reactor的信息 如果发送的数据过大, 则需要将数据进行分片, 限于篇幅, 数据分片, 后续再进行详细讲述 可能存在多个reactor线程同时向同一个worker进程发送数据的情况, 故而Swoole采用SOCK_DGRAM模式与worker进程进行通信, 通过每个数据包的包头, worker进程可以区分出是由哪个reactor线程发送的数据, 也可以知道是哪个请求 worker进程收到reactor发送的数据包后, 进行处理, 处理完成后, 将请求结果发送给主进程 worker进程发送给主进程的数据包, 也会包含一个头部, 当reactor线程收到数据包后, 能够知道对应的reactor线程, 请求的fd等信息 主进程收到worker进程发送的数据包, 这个会触发某个reactor线程进行处理 这个reactor线程并不一定是之前发送请求给worker进程的那个reactor线程 主进程的每个reactor线程都负责监听worker进程发送的数据包, 每个worker发送的数据包只会由一个reactor线程进行监听, 故而只会触发一个reactor线程 reactor线程处理worker进程发送的请求处理结果, 如果是直接发送数据给客户端, 则可以直接发送, 如果需要改变这个这个连接的监听状态(例如close), 则需要先找到监听这个连接的reactor线程, 进而改变这个连接的监听状态(通过调用epoll_ctl) reactor处理线程与reactor监听线程可能并不是同一个线程 reactor监听线程负责监听客户端发送的数据, 进而转发给worker进程 reactor处理线程负责监听worker进程发送给主进程的数据, 进而将数据发送给客户端 四. gdb调试4.1 process模式启动12345678910111213141516171819202122232425262728293031323334//fork manager进程#0 0x00007ffff67dae64 in fork () from /lib64/libc.so.6#1 0x00007ffff553888a in swoole_fork () at /root/code/swoole-src/src/core/base.c:186#2 0x00007ffff556afb8 in swManager_start (serv=serv@entry=0x1353f60) at /root/code/swoole-src/src/server/manager.cc:164#3 0x00007ffff5571dde in swFactoryProcess_start (factory=0x1353ff8) at /root/code/swoole-src/src/server/process.c:198#4 0x00007ffff556ef8b in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:651#5 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#6 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#7 execute_ex (ex=0x7ffff7f850a8) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#8 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#9 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#10 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#11 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#12 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389// pthread_create reactor线程#0 0x00007ffff552e960 in pthread_create@plt () from /usr/local/lib/php/extensions/no-debug-non-zts-20180731/swoole.so#1 0x00007ffff5576959 in swReactorThread_start (serv=0x1353f60) at /root/code/swoole-src/src/server/reactor_thread.c:883#2 0x00007ffff556f006 in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:670#3 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#4 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#5 execute_ex (ex=0x7fffffffab10) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#6 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#7 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#8 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#9 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#10 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389 4.2 base模式启动12345678910111213141516171819//base 模式下的启动#0 0x00007ffff67dae64 in fork () from /lib64/libc.so.6#1 0x00007ffff553888a in swoole_fork () at /root/code/swoole-src/src/core/base.c:186#2 0x00007ffff5558557 in swProcessPool_spawn (pool=pool@entry=0x7ffff2d2a308, worker=0x7ffff2d2a778) at /root/code/swoole-src/src/network/process_pool.c:392#3 0x00007ffff5558710 in swProcessPool_start (pool=0x7ffff2d2a308) at /root/code/swoole-src/src/network/process_pool.c:227#4 0x00007ffff55741cf in swReactorProcess_start (serv=0x1353f60) at /root/code/swoole-src/src/server/reactor_process.cc:176#5 0x00007ffff556f21d in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:666#6 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#7 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#8 execute_ex (ex=0x7ffff2d2a308) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#9 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#10 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#11 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#12 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#13 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389 五. 总结及思考 本文主要介绍了Swoole server的两种模式: base模式、process模式, 详细讲解了两种模式的网络编程模型, 并重点介绍了process模式下, 进程间通信的方式、请求的处理流程等。 process模式下, 为什么不直接在主进程中创建多个线程, 由线程直接进行处理请求(可以避免进程间通信的开销), 而是创建出manager进程, 再由manager进程创建出worker进程, 由worker进程处理请求? 个人觉得可能是php对多线程的支持不是很友好, phper大都也只是进行单线程编程 ZendVM 提供的 TSRM 虽然也是支持多线程环境，但实际上这是一个 按线程隔离内存的方案, 多线程并没有意义 process模式下, 主进程中的每个reactor线程都可以同时处理多个请求, 多个请求是并发处理的, 我们从2个维度看, 从主进程的角度看, 主进程同时处理多个请求, 当一个请求包全部接收完后, 转发给worker进程进行处理 从某个worker进程的角度看, 这个worker进程收到的请求是串行的, 默认情况下, worker进程也是串行处理请求, 如果单个请求阻塞(Swoole的worker进程会回调phper写的事件处理函数, 该函数可能阻塞), 后续的请求也无法处理, 这个就是排头阻塞问题, 这种情况下可以使用Swoole的协程, 通过协程的调度, 单个请求阻塞时, worker进程可以继续处理其他请求。 使用Swoole创建tcp server时, 由于tcp是字节流的协议, 需要分包, 而Swoole在不清楚客户端与服务端通信协议的情况下, 无法进行分包, process模式下, reactor交给worker进程的数据也只能是字节流的, 需要用户自行处理。当然, 一般情况也不需要自行构建协议, 使用tcp server, Swoole已经支持Http, Https等协议。 六. 参考 UNIX网络编程 UNIX环境高级编程 php7 底层实现与源码分析 https://wiki.swoole.com/ https://www.cnblogs.com/welhzh/p/3772164.html https://www.cnblogs.com/JohnABC/p/4079669.html]]></content>
      <categories>
        <category>swoole</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>server</tag>
        <tag>swoole</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置存储概述]]></title>
    <url>%2F2018%2F11%2F13%2Fnginx-config-store-summarize%2F</url>
    <content type="text"><![CDATA[一. 基础nginx的一般配置如下所示:123456789101112131415161718192021222324252627282930...work_porcess: xx;events&#123; ... work_connections xx;&#125;http&#123; //第一级别的配置块 ... upstream xx&#123; //第二级别的配置块 ... &#125; server&#123; //第二级别的配置块 ... location /&#123; //第三级别的配置块 .... location /&#123; //第四级别的配置块 ... &#125; &#125; &#125; server&#123; //第二级别的配置块 ... &#125;&#125; 如何存储上述结构? 由于配置中存在嵌套, 可以使用树型结构进行存储 如何使用上述配置? 从上到下, 按层查找 配置解析核心? 对于块中嵌套块的模块存储(例如http), 核心在于要以块为单位进行分析。例如, 最外层为http块, 属于一级配置; http中的server或者upstream属于二级配置块; server中的location属于三级配置块; 依次类推, 每块都看作一个完整的结构。 由于存在块中嵌套, 内层的块有些配置需要从外层块继承, 有些配置不能继承, 故而每块的配置需要按区域划分。 二级配置块需要放到一级配置块中, 这样才能从一级配置块中找到所有的二级配置块, 以此类推。 由于配置解析是依靠各个模块完成的, 故而配置存储的树形结构中需要以模块为单位存储(也就是每个模块具有自己的存储结构, 可以放在树形结构的某个位置上)。 二. nginx配置存储结构由于结构比较复杂, 此处分为2部分。 第一部分配置存储结构图 图中只给出了http中的server配置, 没有画出server中的location以及location嵌套的location 第二部分配置存储结构图 三. http块存储结构对于http的配置存储, 应当以块为单位进行分析, 总体而言, http块按树形存储 大体上可以分为3个级别(不考虑location中的location), 每个级别的配置分为3块(main块, srv块, loc块) 第二级别的配置继承第一级别的main块配置 第三级别的配置继承第二级别的main块以及srv块配置 第四及其后续级别的配置继承前一级的main块, srv块配置 四. nginx upstream块存储 upstream的配置与server类似, 属于第二级别配置块 upstream的配置块中main块从第一级别的main块继承 五. nginx proxy存储 proxy_pass出现在location块中, 属于第三或者之后级别的配置。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>配置存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx listen指令处理流程]]></title>
    <url>%2F2018%2F11%2F12%2Fnginx-listen-socket%2F</url>
    <content type="text"><![CDATA[一. 基础 nginx源码采用1.15.5 后续部分仅讨论http中的listen配置解析以及优化流程 1.1 概述 假设nginx http模块的配置如下 12345678910111213141516171819202122232425262728293031323334http&#123; server &#123; listen 127.0.0.1:8000; server_name www.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 10.0.1.1:8000; server_name www.news.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 8000; #相当于0.0.0.0:8000 server_name www.tieba.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 127.0.0.1:8000; server_name www.zhidao.baidu.com; location / &#123; root html; index index.html; &#125; &#125;&#125; 端口, 地址, server的关系 端口是指一个端口号, 例如上面的8000端口 地址是ip+port, 例如127.0.0.1:8000, 10.0.1.1:8000, 0.0.0.0:8000, listen后配置的是一个地址。 每个地址可以放到多个server中, 例如上面的127.0.0.1:8000 总而言之, 一个端口可以有多个地址, 每个地址可以有多个server 1.2 存在的问题 是否需要在读取完http块中所有的server才能建立监听套接字, 绑定监听地址? 是的, 因为允许配置通配地址, 故而必须将http块中的server全部读取完后, 才能知道如何建立监听套接字。 一个端口可以对应多个地址, 如何建立监听套接字, 如何绑定地址? 通常情况下, 每个地址只能绑定一次(只考虑tcp协议), 这种情况下, 我们只能选择部分地址创建监听套接字, 绑定监听地址。 当配置中存在通配地址(0.0.0.0:port)时, 只需要创建一个监听套接字, 绑定这个通配地址即可, 但需要能够依据该监听套接字找到该端口配置的其他地址, 这样当客户端发送请求时, 可以根据客户端请求的地址, 找到对应地址下的相关配置。 当配置中不存在通配地址时, 需要对每个地址都创建一个监听套接字, 绑定监听地址。 一个地址多个server的情况下, 如何快速找到客户端请求的server? 比较合适的方案是通过hash表。 为了快速找到客户端请求的server, nginx以server_name为key, 每个server块的配置(可以理解为一个指针, 该指针指向整个server块的配置)为value, 放入到哈希表。 由于server_name中可以出现正则匹配等情况, nginx将server_name具体分为4类进行分别处理(www.baidu.com, *baidu.com, www.baidu\*, ~*baidu)。 1.3 nginx listen解析的流程总体而言分为2步， 将所有http模块内的配置解析完成, 将listen的相关配置暂存(主要存储监听端口以及监听地址)。 根据上一步暂存的监听端口以及监听地址, 创建监听套接字, 绑定监听地址 二. 配置解析nginx http块解析完成后, 会存储配置文件中配置的监听端口以及监听地址, 其核心结构图如下, 总体而言, 结构可以分为3级, 端口-&gt;地址-&gt;server 2.1 源码listen的处理流程: ngx_http_core_listen: 读取配置文件配置 ngx_http_add_listen: 查看之前是否出现过当前监听的端口, 没有则新建, 否则追加 ngx_http_add_address: 查看之前该端口下是否监听过该地址, 没有则新建, 否则追加。 ngx_http_add_server: 查看server之前是否出现过, 没有则新建, 否则报错(重复定义)。 三. 创建监听套接字nginx最终创建的监听套接字及其相关的结构图如下, 每个ngx_listening_t结构对应一个监听套接字, 绑定一个监听地址 每个ngx_listening_t结构后面需要存储地址信息, 地址可能不止一个, 因为这个监听套接字可能绑定的是通配地址, 这个端口下的其他地址都会放在这个监听套接字下。例如, 1.1节的配置中, 只会创建一个ngx_listening_t结构, 其他地址的配置都会放到这个通配地址下。 每个监听地址可能对应多个域名(配置文件中的server_name), 需要将这些域名放到哈希表中, 以供后续使用 总体而言, 结构分为3级, 监听套接字-&gt;监听地址-&gt;server 3.1 源码读取完http块后, 需要创建监听套接字绑定监听地址, 处理函数ngx_http_optimize_servers, 该函数的处理流程: 遍历所有监听端口, 针对每个监听端口, 执行以下3步 对该端口下所有监听地址排序(listen后配置bind的放在前面, 通配地址放在后面) 遍历该端口下的所有地址, 将每个地址配置的所有server, 放到该地址的哈希表中。 为该端口建立监听套接字, 绑定监听地址。 四. 监听套接字的使用 假设此处我们使用epoll作为事件处理模块 epoll在增加事件时, 用户可以使用epoll_event中的data字段, 当事件发生时, 该字段也会带回。 nginx中的epoll_event指向的是ngx_connection_t结构, 事件发生时, 调用ngx_connection_t结构中的读写事件, 负责具体处理事件, 参见下图。12345//c is ngx_connection_trev = c-&gt;read;rev-&gt;hadler(rev);wev = c-&gt;write;wev-&gt;handler(wev); 每个监听套接字对应一个ngx_connection_t, 该结构的读事件回调函数为ngx_event_accept, 当用户发起tcp握手时, 通过ngx_event_accept接受客户端的连接请求。 ngx_event_accept会接受客户端请求, 初始化一个新的ngx_connection_t结构, 并将其加入到epoll中进行监听, 最后会调用ngx_connection_t对应的ngx_listening_t的处理函数(http块对应ngx_http_init_connection, mail块ngx_mail_init_connection, stream块对应ngx_stream_init_connection) 五. 总结 nginx在读取listen相关的配置时, 将结构分为3级, 端口-&gt;地址-&gt;server, 各级都是一对多的关系。 nginx在创建监听套接字时, 将结构分为3级, 监听套接字-&gt;地址-&gt;server, 各级都是一对多的关系。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>nginx</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向代理]]></title>
    <url>%2F2018%2F11%2F09%2Fbrowser-proxy%2F</url>
    <content type="text"><![CDATA[一. 概述 什么是正向代理? 简单来说就是代理客户端请求的服务器。例如, 浏览器中设置代理翻墙等。 正向代理的主要问题? 代理服务器需要知道客户的目标服务器, 例如: 客户一会请求www.baidu.com, 一会请求www.taobao.com, 代理服务器如何获取目标服务器信息。 正向代理问题的解决方案? 客户端按照某种事先约定的协议通知代理服务器, 例如sock5协议 代理服务器能够直接从客户端与服务端交互的上层数据包(tcp之上)获取目标服务器信息。例如: 客户端与目标服务器按照http协议通信, 代理服务器可以直接从数据包中解析目标服务器。这种情况不适用与客户端与目标服务器加密通信的情况, 例如https。 二. socks52.1 socks5基本流程sock5通信的基本流程图如下, 如果不需要认证, 第三步和第四步则不需要。 2.2 socks5总结 socks5主要完成目标地址传递的功能 浏览器与sock5服务器完成tcp握手 浏览器将目标地址通过socks5协议发送给socks5服务器 socks5服务器与目标地址完成tcp握手 浏览器将请求数据发送给socks5服务器, 由其进行转发, 此时可以只做4层tcp代理 三. shadowsocks原理shadowsocks信息交换的基本结构图如下, ss_cli代表shadowsocks客户端, ss_srv代表shadowsocks服务端, 3.1 第一阶段 客户端与ss_cli建立tcp连接 客户端与ss_cli协商sock认证参数, ss_cli给与响应, 默认不进行认证 客户端将目标地址发送给ss_cli, ss_cli给与响应, 默认返回成功 3.2 第二阶段 ss_cli与ss_srv建立tcp连接 ss_cli将客户端发送的目标地址发送给ss_srv, 此时按照ss_cli与ss_srv协商好的数据包格式, 并不需要使用sock协议, 同时还会发送加密解密需要的随机数 ss_srv与目标地址建立tcp连接 3.3 第三阶段 客户端向ss_cli发送tcp数据包 ss_cli将数据包稍作处理, 计算长度, 计算哈希值, 并将这些信息一并发送给ss_srv, 此时的数据包格式是ss_cli与ss_srv约定好的 ss_srv受到数据后, 解析后发送给目标服务器 3.4 第四阶段 ss_srv收到目标服务器返回的数据后, 将数据按约定好的格式转发给ss_cli ss_cli受到数据后, 解析转发给客户 3.5 总结 通过sock5协议, ss_cli可以知道客户端的目标服务器地址 ss_cli与ss_srv可以自行定义协议, 核心是需要将客户端的目标服务器地址以及解密需要的随机变量发送给ss_srv ss_cli, ss_srv不需要也无法知道客户端与目标服务器的具体通信信息。 ss_cli, ss_srv后期是tcp的代理, 不管是https还是http对其而言都是一样的。 3.6 参考 https://www.jianshu.com/p/cbea16a096fb 四. 抓包分析 ss_cli: 127.0.0.1:1080 ss_srv: 204.48.26.173:21500 浏览器与ss_cli同属于一个主机, 通信时延低。ss_cli与ss_srv需要通过网络传输, 时延高。数据包分析中可以利用这一点。 4.1 浏览器与ss_cli通信 tcp握手, 浏览器通过socks5发送目标服务器地址 浏览器与ss_cli之间后续的通信, 此处是https协议 时间突变的部分是由于ss_cli需要ss_srv将目标服务器的应答信息发送回来。 4.2 ss_cli与ss_srv通信二者的通信格式是自行定义的,]]></content>
      <categories>
        <category>网络编程</category>
        <category>代理</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结构体传输]]></title>
    <url>%2F2018%2F11%2F05%2Fstruct-transmission%2F</url>
    <content type="text"><![CDATA[一. 基础 结构体传输基本上有两种方式,序列化(Json,Xml等)以及直接传输结构体。 下面考虑32位系统，直接发送结构体进行传输。 二. 结构体12345struct Data&#123; char v1; int v2; char v3;&#125; 三. 源码3.1 发送方123456789101112131415161718//将data写入bufbool send(char *buf, int bufLen, const Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memcpy(buf, data, sizeof(*data); return true;&#125;//main functionint main(int argc, char **argv)&#123; char sendBuf[256] = &#123;&#125;; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; $ans = send(sendBuf, sizeof(sendBuf), &amp;data); if($ans == false) return -1; //send data&#125; 3.2 接收方123456789101112131415161718//deal databool parseData(char *buf, int bufLen, Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memecpy(buf, data, sizeof(*data)); data.v2 = ntohl(data.v2); return true;&#125;//mainint main(int argc, char **argv)&#123; char recvBuf[256] = &#123;&#125;; //read socket recv data //deal data Data data; $ans = parseData(buf, sizeof(recvBuf), &amp;data); if($ans == false) return -1; //Deal data&#125; 3.3 测试123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;arpa/inet.h&gt;using namespace std;struct Data&#123; char v1; int v2; char v3;&#125;;void parse(char *buf, Data *data)&#123; memcpy(data, buf, sizeof(*data)); data-&gt;v2 = ntohl(data-&gt;v2);&#125;int main (int argc, char **argv)&#123; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; char buf[128] = &#123;&#125;; memcpy(buf, &amp;data, sizeof(data)); Data newData; parse(buf, &amp;newData); std::cout &lt;&lt; newData.v1 &lt;&lt; ' ' &lt;&lt; newData.v2 &lt;&lt; ' ' &lt;&lt; newData.v3 &lt;&lt; std::endl; return 0;&#125;//输出: a, 2, b 四. 注意事项 sizeof(data) = 12; 结构体要考虑对齐,发送方与接收方的对齐方式应该是一致的。 发送方按照网络字节序存储,接收方得到网络字节序的数据后，解析成本机字节序。]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节序与位序]]></title>
    <url>%2F2018%2F11%2F05%2Fnet-byte-order%2F</url>
    <content type="text"><![CDATA[一. 本机字节序 小端: 低位字节存在低地址。 大端: 低位字节存在高地址。 二. 本机位序一般情况下，本机位序与本机的字节序一致。 小端字节序: 低位bit存在低地址。 大端字节序: 低位bit存在高地址。 三. 网络序 网络字节序(大端)，先传送高位字节，再传送低位字节。 在传输一个字节时，先传送低位bit, 再传送高位bit。 注: 指针指向变量或者数组的起始地址，即指向低地址。 四. 代码示例12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main (int argc, char **argv)&#123; union byte_order&#123; int a; unsigned char b[4]; &#125;; byte_order val; val.a = 0x01020304; printf("address 0x%x byte: 0x%x\n", &amp;val.b[0], val.b[0]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[1], val.b[1]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[2], val.b[2]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[3], val.b[3]); struct bit_order&#123; unsigned char a:4; unsigned char b:4; &#125;; unsigned char tmp = 0x04; bit_order *val1 = (bit_order*)&amp;tmp; printf("low bit %d\n", val1-&gt;a); printf("high bit %d\n", val1-&gt;b);&#125;//输出:address 0x56074a68 byte: 0x4address 0x56074a69 byte: 0x3address 0x56074a6a byte: 0x2address 0x56074a6b byte: 0x1low bit 4high bit 0]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>数据传输顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[水平触发与边沿触发]]></title>
    <url>%2F2018%2F11%2F05%2FLT-ET%2F</url>
    <content type="text"><![CDATA[一. 基础1.1 水平触发 基本概念 读缓冲区不为空时, 读事件触发。 写缓冲区不为满时, 写事件触发。 处理流程 accept新的连接, 监听读事件。 读事件到达, 处理读事件。 需要写入数据, 向fd中写数据, 一次无法写完, 开启写事件监听。 写事件到达, 继续写入数据, 写完后关闭写事件。 优缺点 不会遗漏事件, 易编程。 长连接需要写入的数据量大时, 会频繁开启关闭写事件。 1.2 边沿触发 基本概念 读缓冲区状态变化时, 读事件触发, 网卡接受到新数据。 写缓冲区状态变化时, 写事件触发, 网卡发出了新数据。 处理流程 accept新的连接, 同时监听读写事件。 读事件到达, 需要一直读取数据, 直到返回EAGAIN。 写事件到达, 无数据处理则不处理, 有数据待写入则一直写入，直到写完或者返回EAGAIN。 优缺点 不需要频繁开启关闭事件, 效率较高。 读写事件处理不当, 可能导致事件丢失, 编程教复杂。 1.3 选择 概述 对于读事件而言，总体而言, 采用水平触发方式较好。应用程序在读取数据时，可能会一次无法读取全部数据，边沿触发在下一次可能不会触发。如果能够保证一次读取缓存的全部数据，可以采用边沿触发，效率更高, 但同时编程复杂度也高。 对于写事件，当客户端服务端采用短连接或者采用长连接但发送的数据量比较少时(例如: Redis), 采用水平触发即可。当客户端与服务端是长连接并且数据写入的量比较大时(例如: nginx), 采用边沿触发, 因为边沿触发效率更高。 目前，linux不支持读写事件分别设置不同的触发方式，具体采用哪种方式触发，需要根据具体需求。 监听套接字事件设置 监听套接字不需要监听写事件，只需要监听读事件。 监听套接字一般采用水平触发方式。(nginx开启multi_accept时，会把监听套接字所有可读的事件全部读取，此时可以使用边沿触发。但为了保证连接不丢失，nginx仍然采用水平触发) 通信套接字设置 redis对于与客户端通信使用的套接字默认使用水平触发。 nginx对于与客户端通信使用的套接字默认采用边沿触发。 二. 参考 https://blog.csdn.net/dongfuye/article/details/50880251]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F11%2F01%2Fload_balance%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 基础 什么是负载均衡? 当单机提供的并发量不能满足需求时，我们需要多台服务器同时服务。当客户请求到达时，如何为客户选择最合适的服务器?这个问题就是负载均衡问题。 负载均衡主要需要解决的问题是哪些? 从客户端的角度上看，客户需要最快速的得到服务器的相应，负载均衡时需要找出能最快相应客户需求的服务器进行服务。 从服务端来看如何使得每台服务器都能达到较高的利用率，最大限制的为用户提供快速、可靠的服务是服务端需要考虑的主要问题。 1.2 负载均衡分类 硬件 F5 软件 dns负载均衡 LVS负载均衡(4层) nginx, haproxy(7层) 二. F5负载均衡 F5是一家美国的公司，该公司生产一些硬件设备可以作为负载均衡器使用(例如:big-ip), 本文后续部分所说的F5是指其负载均衡器产品。 不同的产品实现的功能不一致，具体情况需要根据产品说明书。 F5可以在4-7层内做负载均衡，用户可以根据需求进行配置。 由于F5可以做7层负载均衡，故而可以实现会话管理，http处理等。 2.1 数据转发模式 standard类型, 这种模式下，客户端与F5服务器建立连接，F5服务器与真实服务器建立连接，F5服务器将客户需求转发给真实服务器，并将真实服务器的相应转发给客户端，此时F5可以查看请求和相应的所有信息。 四层转发模式(performance L4), 这种模式下，F5只处理4层以下的数据。客户端将数据发送给F5, F5仅将数据转发给真实服务器，包括TCP的握手数据包以及挥手数据包，真实服务器需要先将数据发送给F5服务器，F5将其转发给客户端。 路由模式, 这种模式与LVS的DR模式类似。 … 2.2 负载均衡算法 轮询，加权轮询。 源地址哈希 … 2.3 小结F5的优势在于功能强大，并发量高，能满足客户的大多数需求，但其成本较高，一般大型国企可能会使用。 2.4 参考 https://f5.com/zh https://www.jianshu.com/p/2b55aa4c21e2 https://wenku.baidu.com/view/450b8643cc7931b765ce15c1.html 三. dns负载均衡 dns负载均衡由dns服务提供厂商提供。 最初的dns负载均衡提供简单轮询，不能根据客户端或者服务端状态进行选择。 目前，有些dns服务厂商可以提供智能dns服务，用户可以设置负载均衡方案，例如：根据客户端ip地址，选择就近的服务器。 对于目前大多数的公司而言，为了更好的服务用户，通常会使用dns负载均衡，将用户按照就近原则，分配到某个集群服务器上。之后，集群内再采用其他的负载均衡方案。 四. Linux Virtual Server(LVS) LVS通过修改数据包Ip地址，Mac地址实现负载均衡。 LVS由ipvs(内核中), ipvsadm(用户态)组成。LVS需要理解tcp，ip头部。 当tcp握手信号，SYN数据包达到时，ipvs选择一个后端服务器，将数据包进行转发。在此之后，所有包含相同的ip，tcp头部的数据包都会被转发到之前选择的服务器上。很明显，ipvs无法感知数据包内容。 4.1 分类 LVS-NAT LVS-DR LVS-TUN 4.2 基本原理4.2.1 LVS-DRLVS-DR模式的基本原理如下图所示: 4.2.2 LVS-NATLVS-NAT模式的基本原理如下图所示: 4.3 负载均衡算法4.3.1 静态算法 轮询(Round Robin, RR) 加权轮询(Weight Round Robin, WRR) 源地址Hash(Source Hash, SH) 目的地址Hash(Destination Hash, DH), 可以设置多个VIP 4.3.2 动态算法 最少连接(Least Connections, LC)，找出当前连接数最小的服务器 加权最少连接(Weighted Least Connections, WLC) 最短期望延迟(Shortest Expected Delay Scheduling, SED) 基于WLC。例如: 现有A, B, C三台服务器，权重分别为100,200,300，当前的连接数分别为1,2,3,下一个连接到达时，通过计算期望时延选择服务器(1+1)/100, (2+1)/200, (3+1)/300, 故而选择C服务器。 永不排队(Never Queue Scheduling, NQ)， 改进的sed, 如果某台服务器连接数为0，直接连接过去，不在进行sed计算。 基于局部性的最少连接(locality-Based Least Connections, LBLC)，根据目标ip, 找出目标ip最近使用的服务器，如果服务器存在并且负载没有大于一个阈值，则将新的连接分配到这个服务器上，否则按照最少连接找出一个服务器处理该请求。 带复制的基于局部性最少连接(Locality-Based Least Connections with Replication, LBLCR)，根据目标ip，维护一个服务器组，每次从组中挑选服务器，如果服务器不可以处理，则从所有服务器中按照最少连接挑选出一台服务器，并将其加入到目标ip的处理组服务器中。 4.3 参考 https://liangshuang.name/2017/11/19/lvs/ 五. Nginx Load Balance nginx负载均衡工作在7层，它会与client、upstream分别建立tcp连接，nginx需要维护这两个连接的状态。 nginx的stream模块可以用于4层负载均衡，但一般很少使用。 5.1 基本原理nginx做7层负载均衡的基本原理如下图所示: 5.2 负载均衡算法 轮询(默认) 加权轮询 源ip哈希 响应时间 url 哈希]]></content>
      <categories>
        <category>架构</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>分布式技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx内存管理]]></title>
    <url>%2F2018%2F10%2F31%2Fnginx_memory_manage%2F</url>
    <content type="text"><![CDATA[一. 概述 应用程序的内存可以简单分为堆内存，栈内存。对于栈内存而言，在函数编译时，编译器会插入移动栈当前指针位置的代码，实现栈空间的自管理。而对于堆内存，通常需要程序员进行管理。我们通常说的内存管理亦是只堆空间内存管理。 对于内存，我们的使用可以简化为3步，申请内存、使用内存、释放内存。申请内存，使用内存通常需要程序员显示操作，释放内存却并不一定需要程序员显示操作，目前很多的高级语言提供了垃圾回收机制，可以自行选择时机释放内存，例如: Go、Java已经实现垃圾回收, C语言目前尚未实现垃圾回收，C++中可以通过智能指针达到垃圾回收的目的。 除了语言层面的内存管理外，有时我们需要在程序中自行管理内存，总体而言，对于内存管理，我认为主要是解决以下问题: 用户申请内存时，如何快速查找到满足用户需求的内存块？ 用户释放内存时，如何避免内存碎片化？ 无论是语言层面实现的内存管理还是应用程序自行实现的内存管理，大都将内存按照大小分为几种，每种采用不同的管理模式。常见的分类是按照2的整数次幂分，将不同种类的内存通过链表链接，查询时，从相应大小的链表中寻找，如果找不到，则可以考虑从更大块内存中，拿取一块，将其分为多个小点的内存。当然，对于特别大的内存，语言层面的内存管理可以直接调用内存管理相关的系统调用，应用层面的内存管理则可以直接使用语言层面的内存管理。 nginx内存管理整体可以分为2个部分， 第一部分是常规的内存池，用于进程平时所需的内存管理； 第二部分是共享内存的管理。总体而言，共享内存教内存池要复杂的多。 二. nginx内存池管理2.1 说明 本部分使用的nginx版本为1.15.3 具体源码参见src/core/ngx_palloc.c文件 2.2 nginx实现2.2.1 使用流程nginx内存池的使用较为简单,可以分为3步， 调用ngx_create_pool函数获取ngx_pool_t指针。 12//size代表ngx_pool_t一块的大小ngx_pool_t* ngx_create_pool(size_t size, ngx_log_t *log) 调用ngx_palloc申请内存使用 12//从pool中申请size大小的内存void* ngx_palloc(ngx_pool_t *pool, size_t size) 释放内存(可以释放大块内存或者释放整个内存池) 1234//释放从pool中申请的大块内存ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p)//释放整个内存池void ngx_destroy_pool(ngx_pool_t *pool) 2.2.2 具体实现 如下图所示，nginx将内存分为2种，一种是小内存，一种是大内存，当申请的空间大于pool-&gt;max时，我们认为是大内存空间，否则是小内存空间。12//创建内存池的参数size减去头部管理结构ngx_pool_t的大小pool-&gt;max = size - sizeof(ngx_pool_t); 对于小块内存空间, nginx首先查看当前内存块待分配的空间中，是否能够满足用户需求，如果可以，则直接将这部分内存返回。如果不能满足用户需求，则需要重新申请一个内存块，申请的内存块与当前块空间大小相同，将新申请的内存块通过链表链接到上一个内存块，从新的内存块中分配用户所需的内存。 小块内存并不释放，用户申请后直接使用，即使后期不再使用也不需要释放该内存。由于用户有时并不知道自己使用的内存块是大是小，此时也可以调用ngx_pfree函数释放该空间，该函数会从大空间链表中查找内存，找到则释放内存。对于小内存而言，并未做任何处理。 对于大块内存, nginx会将这些内存放到链表中存储，通过pool-&gt;large进行管理。值得注意的是，用户管理大内存的ngx_pool_large_t结构是从本内存池的小块内存中申请而来，也就意味着无法释放这些内存，nginx则是直接复用ngx_pool_large_t结构体。当用户需要申请大内存空间时，利用c函数库malloc申请空间，然后将其挂载某个ngx_pool_large_t结构体上。nginx在需要一个新的ngx_pool_large_t结构时，会首先pool-&gt;large链表的前3个元素中，查看是否有可用的,如果有则直接使用，否则新建ngx_pool_large_t结构。 三. nginx共享内存管理3.1 说明 本部分使用的nginx版本是1.15.3 本部分源码详见src/core/ngx_slab.c, src/core/ngx_shmtx.c nginx共享内存内容相对较多，本文仅做简单概述。 3.2 直接使用共享内存3.2.1 基础 nginx中需要创建互斥锁，用于后面多进程同步使用。除此之外，nginx可能需要一些统计信息，例如设置(stat_stub),对于这些变量，我们并不需要特意管理，只需要开辟共享空间后，直接使用即可。 设置stat_stub后所需的统计信息，亦是放到共享内存中，我们此处仅以nginx中的互斥锁进行说明。 3.2.2 nginx互斥锁的实现 nginx互斥锁，有两种方案，当系统支持原子操作时，采用原子操作，不支持时采用文件锁。本节源码见ngx_event_module_init函数。 下图为文件锁实现互斥锁的示意图。 下图为原子操作实现互斥锁的示意图。 问题 reload时，新启动的master向老的master发送信号后直接退出，旧的master,重新加载配置(ngx_init_cycle函数), 新创建工作进程, 新的工作进程与旧的工作进程使用的锁是相同的。 平滑升级时, 旧的master会创建新的master, 新的master会继承旧的master监听的端口(通过环境变量传递监听套接字对应的fd)，新的进程并没有重新绑定监听端口。可能存在新老worker同时监听某个端口的情况，此时操作系统会保证只会有一个进程处理该事件(虽然epoll_wait都会被唤醒)。 3.3 通过slab管理共享内存 nginx允许各个模块开辟共享空间以供使用,例如ngx_http_limit_conn_module模块。 nginx共享内存管理的基本思想有: 将内存按照页进行分配，每页的大小相同, 此处设为page_size。 将内存块按照2的整数次幂进行划分, 最小为8bit, 最大为page_size/2。例如，假设每页大小为4Kb, 则将内存分为8, 16, 32, 64, 128, 256, 512, 1024, 2048共9种，每种对应一个slot, 此时slots数组的大小n即为9。申请小块内存(申请内存大小size &lt;= page_size/2)时，直接给用户这9种中的一种，例如，需要30bit时，找大小为32的内存块提供给用户。 每个页只会划分一种类型的内存块。例如，某次申请内存时，现有内存无法满足要求，此时会使用一个新的页，则这个新页此后只会分配这种大小的内存。 通过双向链表将所有空闲的页连接。图中ngx_slab_pool_t中的free变量即使用来链接空闲页的。 通过slots数组将所有小块内存所使用的页链接起来。 对于大于等于页面大小的空间请求，计算所需页数，找到连续的空闲页，将空闲页的首页地址返回给客户使用，通过每页的管理结构ngx_slab_page_t进行标识。 所有页面只会有3中状态，空闲、未满、已满。空闲，未满都是通过双向链表进行整合，已满页面则不存在与任何页面，当空间被释放时，会将其加入到某个链表。 nginx共享内存的基本结构图如下: 在上图中，除了最右侧的ngx_slab_pool_t接口开始的一段内存位于共享内存区外，其他内存都不是共享内存。 共享内存最终是从page中分配而来。]]></content>
      <categories>
        <category>nginx</category>
        <category>内存管理</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
