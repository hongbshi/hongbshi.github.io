<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[nginx 配置存储概述]]></title>
    <url>%2F2018%2F11%2F13%2Fnginx-config-store-summarize%2F</url>
    <content type="text"><![CDATA[一. 基础nginx的一般配置如下所示:123456789101112131415161718192021222324252627282930...work_porcess: xx;events&#123; ... work_connections xx;&#125;http&#123; //第一级别的配置块 ... upstream xx&#123; //第二级别的配置块 ... &#125; server&#123; //第二级别的配置块 ... location /&#123; //第三级别的配置块 .... location /&#123; //第四级别的配置块 ... &#125; &#125; &#125; server&#123; //第二级别的配置块 ... &#125;&#125; 如何存储上述结构? 由于配置中存在嵌套, 可以使用树型结构进行存储 如何使用上述配置? 从上到下, 按层查找 配置解析核心? 对于块中嵌套块的模块存储(例如http), 核心在于要以块为单位进行分析。例如, 最外层为http块, 属于一级配置; http中的server或者upstream属于二级配置块; server中的location属于三级配置块; 依次类推, 每块都看作一个完整的结构。 由于存在块中嵌套, 内层的块有些配置需要从外层块继承, 有些配置不能继承, 故而每块的配置需要按区域划分。 二级配置块需要放到一级配置块中, 这样才能从一级配置块中找到所有的二级配置块, 以此类推。 由于配置解析是依靠各个模块完成的, 故而配置存储的树形结构中需要以模块为单位存储(也就是每个模块具有自己的存储结构, 可以放在树形结构的某个位置上)。 二. nginx配置存储结构由于结构比较复杂, 此处分为2部分。 第一部分配置存储结构图 图中只给出了http中的server配置, 没有画出server中的location以及location嵌套的location 第二部分配置存储结构图 三. http块存储结构对于http的配置存储, 应当以块为单位进行分析, 总体而言, http块按树形存储 大体上可以分为3个级别(不考虑location中的location), 每个级别的配置分为3块(main块, srv块, loc块) 第二级别的配置继承第一级别的main块配置 第三级别的配置继承第二级别的main块以及srv块配置 第四及其后续级别的配置继承前一级的main块, srv块配置 四. nginx upstream块存储 upstream的配置与server类似, 属于第二级别配置块 upstream的配置块中main块从第一级别的main块继承 五. nginx proxy存储 proxy_pass出现在location块中, 属于第三或者之后级别的配置。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>配置存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx listen指令处理流程]]></title>
    <url>%2F2018%2F11%2F12%2Fnginx-listen-socket%2F</url>
    <content type="text"><![CDATA[一. 基础 nginx源码采用1.15.5 后续部分仅讨论http中的listen配置解析以及优化流程 1.1 概述 假设nginx http模块的配置如下 12345678910111213141516171819202122232425262728293031323334http&#123; server &#123; listen 127.0.0.1:8000; server_name www.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 10.0.1.1:8000; server_name www.news.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 8000; #相当于0.0.0.0:8000 server_name www.tieba.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 127.0.0.1:8000; server_name www.zhidao.baidu.com; location / &#123; root html; index index.html; &#125; &#125;&#125; 端口, 地址, server的关系 端口是指一个端口号, 例如上面的8000端口 地址是ip+port, 例如127.0.0.1:8000, 10.0.1.1:8000, 0.0.0.0:8000, listen后配置的是一个地址。 每个地址可以放到多个server中, 例如上面的127.0.0.1:8000 总而言之, 一个端口可以有多个地址, 每个地址可以有多个server 1.2 存在的问题 是否需要在读取完http块中所有的server才能建立监听套接字, 绑定监听地址? 是的, 因为允许配置通配地址, 故而必须将http块中的server全部读取完后, 才能知道如何建立监听套接字。 一个端口可以对应多个地址, 如何建立监听套接字, 如何绑定地址? 通常情况下, 每个地址只能绑定一次(只考虑tcp协议), 这种情况下, 我们只能选择部分地址创建监听套接字, 绑定监听地址。 当配置中存在通配地址(0.0.0.0:port)时, 只需要创建一个监听套接字, 绑定这个通配地址即可, 但需要能够依据该监听套接字找到该端口配置的其他地址, 这样当客户端发送请求时, 可以根据客户端请求的地址, 找到对应地址下的相关配置。 当配置中不存在通配地址时, 需要对每个地址都创建一个监听套接字, 绑定监听地址。 一个地址多个server的情况下, 如何快速找到客户端请求的server? 比较合适的方案是通过hash表。 为了快速找到客户端请求的server, nginx以server_name为key, 每个server块的配置(可以理解为一个指针, 该指针指向整个server块的配置)为value, 放入到哈希表。 由于server_name中可以出现正则匹配等情况, nginx将server_name具体分为4类进行分别处理(www.baidu.com, *baidu.com, www.baidu\*, ~*baidu)。 1.3 nginx listen解析的流程总体而言分为2步， 将所有http模块内的配置解析完成, 将listen的相关配置暂存(主要存储监听端口以及监听地址)。 根据上一步暂存的监听端口以及监听地址, 创建监听套接字, 绑定监听地址 二. 配置解析nginx http块解析完成后, 会存储配置文件中配置的监听端口以及监听地址, 其核心结构图如下, 总体而言, 结构可以分为3级, 端口-&gt;地址-&gt;server 2.1 源码listen的处理流程: ngx_http_core_listen: 读取配置文件配置 ngx_http_add_listen: 查看之前是否出现过当前监听的端口, 没有则新建, 否则追加 ngx_http_add_address: 查看之前该端口下是否监听过该地址, 没有则新建, 否则追加。 ngx_http_add_server: 查看server之前是否出现过, 没有则新建, 否则报错(重复定义)。 三. 创建监听套接字nginx最终创建的监听套接字及其相关的结构图如下, 每个ngx_listening_t结构对应一个监听套接字, 绑定一个监听地址 每个ngx_listening_t结构后面需要存储地址信息, 地址可能不止一个, 因为这个监听套接字可能绑定的是通配地址, 这个端口下的其他地址都会放在这个监听套接字下。例如, 1.1节的配置中, 只会创建一个ngx_listening_t结构, 其他地址的配置都会放到这个通配地址下。 每个监听地址可能对应多个域名(配置文件中的server_name), 需要将这些域名放到哈希表中, 以供后续使用 总体而言, 结构分为3级, 监听套接字-&gt;监听地址-&gt;server 3.1 源码读取完http块后, 需要创建监听套接字绑定监听地址, 处理函数ngx_http_optimize_servers, 该函数的处理流程: 遍历所有监听端口, 针对每个监听端口, 执行以下3步 对该端口下所有监听地址排序(listen后配置bind的放在前面, 通配地址放在后面) 遍历该端口下的所有地址, 将每个地址配置的所有server, 放到该地址的哈希表中。 为该端口建立监听套接字, 绑定监听地址。 四. 监听套接字的使用 假设此处我们使用epoll作为事件处理模块 epoll在增加事件时, 用户可以使用epoll_event中的data字段, 当事件发生时, 该字段也会带回。 nginx中的epoll_event指向的是ngx_connection_t结构, 事件发生时, 调用ngx_connection_t结构中的读写事件, 负责具体处理事件, 参见下图。12345//c is ngx_connection_trev = c-&gt;read;rev-&gt;hadler(rev);wev = c-&gt;write;wev-&gt;handler(wev); 每个监听套接字对应一个ngx_connection_t, 该结构的读事件回调函数为ngx_event_accept, 当用户发起tcp握手时, 通过ngx_event_accept接受客户端的连接请求。 ngx_event_accept会接受客户端请求, 初始化一个新的ngx_connection_t结构, 并将其加入到epoll中进行监听, 最后会调用ngx_connection_t对应的ngx_listening_t的处理函数(http块对应ngx_http_init_connection, mail块ngx_mail_init_connection, stream块对应ngx_stream_init_connection) 五. 总结 nginx在读取listen相关的配置时, 将结构分为3级, 端口-&gt;地址-&gt;server, 各级都是一对多的关系。 nginx在创建监听套接字时, 将结构分为3级, 监听套接字-&gt;地址-&gt;server, 各级都是一对多的关系。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>nginx</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向代理]]></title>
    <url>%2F2018%2F11%2F09%2Fbrowser-proxy%2F</url>
    <content type="text"><![CDATA[一. 概述 什么是正向代理? 简单来说就是代理客户端请求的服务器。例如, 浏览器中设置代理翻墙等。 正向代理的主要问题? 代理服务器需要知道客户的目标服务器, 例如: 客户一会请求www.baidu.com, 一会请求www.taobao.com, 代理服务器如何获取目标服务器信息。 正向代理问题的解决方案? 客户端按照某种事先约定的协议通知代理服务器, 例如sock5协议 代理服务器能够直接从客户端与服务端交互的上层数据包(tcp之上)获取目标服务器信息。例如: 客户端与目标服务器按照http协议通信, 代理服务器可以直接从数据包中解析目标服务器。这种情况不适用与客户端与目标服务器加密通信的情况, 例如https。 二. socks52.1 socks5基本流程sock5通信的基本流程图如下, 如果不需要认证, 第三步和第四步则不需要。 2.2 socks5总结 socks5主要完成目标地址传递的功能 浏览器与sock5服务器完成tcp握手 浏览器将目标地址通过socks5协议发送给socks5服务器 socks5服务器与目标地址完成tcp握手 浏览器将请求数据发送给socks5服务器, 由其进行转发, 此时可以只做4层tcp代理 三. shadowsocks原理shadowsocks信息交换的基本结构图如下, ss_cli代表shadowsocks客户端, ss_srv代表shadowsocks服务端, 3.1 第一阶段 客户端与ss_cli建立tcp连接 客户端与ss_cli协商sock认证参数, ss_cli给与响应, 默认不进行认证 客户端将目标地址发送给ss_cli, ss_cli给与响应, 默认返回成功 3.2 第二阶段 ss_cli与ss_srv建立tcp连接 ss_cli将客户端发送的目标地址发送给ss_srv, 此时按照ss_cli与ss_srv协商好的数据包格式, 并不需要使用sock协议, 同时还会发送加密解密需要的随机数 ss_srv与目标地址建立tcp连接 3.3 第三阶段 客户端向ss_cli发送tcp数据包 ss_cli将数据包稍作处理, 计算长度, 计算哈希值, 并将这些信息一并发送给ss_srv, 此时的数据包格式是ss_cli与ss_srv约定好的 ss_srv受到数据后, 解析后发送给目标服务器 3.4 第四阶段 ss_srv收到目标服务器返回的数据后, 将数据按约定好的格式转发给ss_cli ss_cli受到数据后, 解析转发给客户 3.5 总结 通过sock5协议, ss_cli可以知道客户端的目标服务器地址 ss_cli与ss_srv可以自行定义协议, 核心是需要将客户端的目标服务器地址以及解密需要的随机变量发送给ss_srv ss_cli, ss_srv不需要也无法知道客户端与目标服务器的具体通信信息。 ss_cli, ss_srv后期是tcp的代理, 不管是https还是http对其而言都是一样的。 3.6 参考 https://www.jianshu.com/p/cbea16a096fb 四. 抓包分析 ss_cli: 127.0.0.1:1080 ss_srv: 204.48.26.173:21500 浏览器与ss_cli同属于一个主机, 通信时延低。ss_cli与ss_srv需要通过网络传输, 时延高。数据包分析中可以利用这一点。 4.1 浏览器与ss_cli通信 tcp握手, 浏览器通过socks5发送目标服务器地址 浏览器与ss_cli之间后续的通信, 此处是https协议 时间突变的部分是由于ss_cli需要ss_srv将目标服务器的应答信息发送回来。 4.2 ss_cli与ss_srv通信二者的通信格式是自行定义的,]]></content>
      <categories>
        <category>网络编程</category>
        <category>代理</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结构体传输]]></title>
    <url>%2F2018%2F11%2F05%2Fstruct-transmission%2F</url>
    <content type="text"><![CDATA[一. 基础 结构体传输基本上有两种方式,序列化(Json,Xml等)以及直接传输结构体。 下面考虑32位系统，直接发送结构体进行传输。 二. 结构体12345struct Data&#123; char v1; int v2; char v3;&#125; 三. 源码3.1 发送方123456789101112131415161718//将data写入bufbool send(char *buf, int bufLen, const Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memcpy(buf, data, sizeof(*data); return true;&#125;//main functionint main(int argc, char **argv)&#123; char sendBuf[256] = &#123;&#125;; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; $ans = send(sendBuf, sizeof(sendBuf), &amp;data); if($ans == false) return -1; //send data&#125; 3.2 接收方123456789101112131415161718//deal databool parseData(char *buf, int bufLen, Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memecpy(buf, data, sizeof(*data)); data.v2 = ntohl(data.v2); return true;&#125;//mainint main(int argc, char **argv)&#123; char recvBuf[256] = &#123;&#125;; //read socket recv data //deal data Data data; $ans = parseData(buf, sizeof(recvBuf), &amp;data); if($ans == false) return -1; //Deal data&#125; 3.3 测试123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;arpa/inet.h&gt;using namespace std;struct Data&#123; char v1; int v2; char v3;&#125;;void parse(char *buf, Data *data)&#123; memcpy(data, buf, sizeof(*data)); data-&gt;v2 = ntohl(data-&gt;v2);&#125;int main (int argc, char **argv)&#123; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; char buf[128] = &#123;&#125;; memcpy(buf, &amp;data, sizeof(data)); Data newData; parse(buf, &amp;newData); std::cout &lt;&lt; newData.v1 &lt;&lt; ' ' &lt;&lt; newData.v2 &lt;&lt; ' ' &lt;&lt; newData.v3 &lt;&lt; std::endl; return 0;&#125;//输出: a, 2, b 四. 注意事项 sizeof(data) = 12; 结构体要考虑对齐,发送方与接收方的对齐方式应该是一致的。 发送方按照网络字节序存储,接收方得到网络字节序的数据后，解析成本机字节序。]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节序与位序]]></title>
    <url>%2F2018%2F11%2F05%2Fnet-byte-order%2F</url>
    <content type="text"><![CDATA[一. 本机字节序 小端: 低位字节存在低地址。 大端: 低位字节存在高地址。 二. 本机位序一般情况下，本机位序与本机的字节序一致。 小端字节序: 低位bit存在低地址。 大端字节序: 低位bit存在高地址。 三. 网络序 网络字节序(大端)，先传送高位字节，再传送低位字节。 在传输一个字节时，先传送低位bit, 再传送高位bit。 注: 指针指向变量或者数组的起始地址，即指向低地址。 四. 代码示例12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main (int argc, char **argv)&#123; union byte_order&#123; int a; unsigned char b[4]; &#125;; byte_order val; val.a = 0x01020304; printf("address 0x%x byte: 0x%x\n", &amp;val.b[0], val.b[0]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[1], val.b[1]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[2], val.b[2]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[3], val.b[3]); struct bit_order&#123; unsigned char a:4; unsigned char b:4; &#125;; unsigned char tmp = 0x04; bit_order *val1 = (bit_order*)&amp;tmp; printf("low bit %d\n", val1-&gt;a); printf("high bit %d\n", val1-&gt;b);&#125;//输出:address 0x56074a68 byte: 0x4address 0x56074a69 byte: 0x3address 0x56074a6a byte: 0x2address 0x56074a6b byte: 0x1low bit 4high bit 0]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>数据传输顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[水平触发与边沿触发]]></title>
    <url>%2F2018%2F11%2F05%2FLT-ET%2F</url>
    <content type="text"><![CDATA[一. 基础1.1 水平触发 基本概念 读缓冲区不为空时, 读事件触发。 写缓冲区不为满时, 写事件触发。 处理流程 accept新的连接, 监听读事件。 读事件到达, 处理读事件。 需要写入数据, 向fd中写数据, 一次无法写完, 开启写事件监听。 写事件到达, 继续写入数据, 写完后关闭写事件。 优缺点 不会遗漏事件, 易编程。 长连接需要写入的数据量大时, 会频繁开启关闭写事件。 1.2 边沿触发 基本概念 读缓冲区状态变化时, 读事件触发, 网卡接受到新数据。 写缓冲区状态变化时, 写事件触发, 网卡发出了新数据。 处理流程 accept新的连接, 同时监听读写事件。 读事件到达, 需要一直读取数据, 直到返回EAGAIN。 写事件到达, 无数据处理则不处理, 有数据待写入则一直写入，直到写完或者返回EAGAIN。 优缺点 不需要频繁开启关闭事件, 效率较高。 读写事件处理不当, 可能导致事件丢失, 编程教复杂。 1.3 选择 概述 对于读事件而言，总体而言, 采用水平触发方式较好。应用程序在读取数据时，可能会一次无法读取全部数据，边沿触发在下一次可能不会触发。如果能够保证一次读取缓存的全部数据，可以采用边沿触发，效率更高, 但同时编程复杂度也高。 对于写事件，当客户端服务端采用短连接或者采用长连接但发送的数据量比较少时(例如: Redis), 采用水平触发即可。当客户端与服务端是长连接并且数据写入的量比较大时(例如: nginx), 采用边沿触发, 因为边沿触发效率更高。 目前，linux不支持读写事件分别设置不同的触发方式，具体采用哪种方式触发，需要根据具体需求。 监听套接字事件设置 监听套接字不需要监听写事件，只需要监听读事件。 监听套接字一般采用水平触发方式。(nginx开启multi_accept时，会把监听套接字所有可读的事件全部读取，此时可以使用边沿触发。但为了保证连接不丢失，nginx仍然采用水平触发) 通信套接字设置 redis对于与客户端通信使用的套接字默认使用水平触发。 nginx对于与客户端通信使用的套接字默认采用边沿触发。 二. 参考 https://blog.csdn.net/dongfuye/article/details/50880251]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F11%2F01%2Fload_balance%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 基础 什么是负载均衡? 当单机提供的并发量不能满足需求时，我们需要多台服务器同时服务。当客户请求到达时，如何为客户选择最合适的服务器?这个问题就是负载均衡问题。 负载均衡主要需要解决的问题是哪些? 从客户端的角度上看，客户需要最快速的得到服务器的相应，负载均衡时需要找出能最快相应客户需求的服务器进行服务。 从服务端来看如何使得每台服务器都能达到较高的利用率，最大限制的为用户提供快速、可靠的服务是服务端需要考虑的主要问题。 1.2 负载均衡分类 硬件 F5 软件 dns负载均衡 LVS负载均衡(4层) nginx, haproxy(7层) 二. F5负载均衡 F5是一家美国的公司，该公司生产一些硬件设备可以作为负载均衡器使用(例如:big-ip), 本文后续部分所说的F5是指其负载均衡器产品。 不同的产品实现的功能不一致，具体情况需要根据产品说明书。 F5可以在4-7层内做负载均衡，用户可以根据需求进行配置。 由于F5可以做7层负载均衡，故而可以实现会话管理，http处理等。 2.1 数据转发模式 standard类型, 这种模式下，客户端与F5服务器建立连接，F5服务器与真实服务器建立连接，F5服务器将客户需求转发给真实服务器，并将真实服务器的相应转发给客户端，此时F5可以查看请求和相应的所有信息。 四层转发模式(performance L4), 这种模式下，F5只处理4层以下的数据。客户端将数据发送给F5, F5仅将数据转发给真实服务器，包括TCP的握手数据包以及挥手数据包，真实服务器需要先将数据发送给F5服务器，F5将其转发给客户端。 路由模式, 这种模式与LVS的DR模式类似。 … 2.2 负载均衡算法 轮询，加权轮询。 源地址哈希 … 2.3 小结F5的优势在于功能强大，并发量高，能满足客户的大多数需求，但其成本较高，一般大型国企可能会使用。 2.4 参考 https://f5.com/zh https://www.jianshu.com/p/2b55aa4c21e2 https://wenku.baidu.com/view/450b8643cc7931b765ce15c1.html 三. dns负载均衡 dns负载均衡由dns服务提供厂商提供。 最初的dns负载均衡提供简单轮询，不能根据客户端或者服务端状态进行选择。 目前，有些dns服务厂商可以提供智能dns服务，用户可以设置负载均衡方案，例如：根据客户端ip地址，选择就近的服务器。 对于目前大多数的公司而言，为了更好的服务用户，通常会使用dns负载均衡，将用户按照就近原则，分配到某个集群服务器上。之后，集群内再采用其他的负载均衡方案。 四. Linux Virtual Server(LVS) LVS通过修改数据包Ip地址，Mac地址实现负载均衡。 LVS由ipvs(内核中), ipvsadm(用户态)组成。LVS需要理解tcp，ip头部。 当tcp握手信号，SYN数据包达到时，ipvs选择一个后端服务器，将数据包进行转发。在此之后，所有包含相同的ip，tcp头部的数据包都会被转发到之前选择的服务器上。很明显，ipvs无法感知数据包内容。 4.1 分类 LVS-NAT LVS-DR LVS-TUN 4.2 基本原理4.2.1 LVS-DRLVS-DR模式的基本原理如下图所示: 4.2.2 LVS-NATLVS-NAT模式的基本原理如下图所示: 4.3 负载均衡算法4.3.1 静态算法 轮询(Round Robin, RR) 加权轮询(Weight Round Robin, WRR) 源地址Hash(Source Hash, SH) 目的地址Hash(Destination Hash, DH), 可以设置多个VIP 4.3.2 动态算法 最少连接(Least Connections, LC)，找出当前连接数最小的服务器 加权最少连接(Weighted Least Connections, WLC) 最短期望延迟(Shortest Expected Delay Scheduling, SED) 基于WLC。例如: 现有A, B, C三台服务器，权重分别为100,200,300，当前的连接数分别为1,2,3,下一个连接到达时，通过计算期望时延选择服务器(1+1)/100, (2+1)/200, (3+1)/300, 故而选择C服务器。 永不排队(Never Queue Scheduling, NQ)， 改进的sed, 如果某台服务器连接数为0，直接连接过去，不在进行sed计算。 基于局部性的最少连接(locality-Based Least Connections, LBLC)，根据目标ip, 找出目标ip最近使用的服务器，如果服务器存在并且负载没有大于一个阈值，则将新的连接分配到这个服务器上，否则按照最少连接找出一个服务器处理该请求。 带复制的基于局部性最少连接(Locality-Based Least Connections with Replication, LBLCR)，根据目标ip，维护一个服务器组，每次从组中挑选服务器，如果服务器不可以处理，则从所有服务器中按照最少连接挑选出一台服务器，并将其加入到目标ip的处理组服务器中。 4.3 参考 https://liangshuang.name/2017/11/19/lvs/ 五. Nginx Load Balance nginx负载均衡工作在7层，它会与client、upstream分别建立tcp连接，nginx需要维护这两个连接的状态。 nginx的stream模块可以用于4层负载均衡，但一般很少使用。 5.1 基本原理nginx做7层负载均衡的基本原理如下图所示: 5.2 负载均衡算法 轮询(默认) 加权轮询 源ip哈希 响应时间 url 哈希]]></content>
      <categories>
        <category>架构</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>分布式技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx内存管理]]></title>
    <url>%2F2018%2F10%2F31%2Fnginx_memory_manage%2F</url>
    <content type="text"><![CDATA[一. 概述 应用程序的内存可以简单分为堆内存，栈内存。对于栈内存而言，在函数编译时，编译器会插入移动栈当前指针位置的代码，实现栈空间的自管理。而对于堆内存，通常需要程序员进行管理。我们通常说的内存管理亦是只堆空间内存管理。 对于内存，我们的使用可以简化为3步，申请内存、使用内存、释放内存。申请内存，使用内存通常需要程序员显示操作，释放内存却并不一定需要程序员显示操作，目前很多的高级语言提供了垃圾回收机制，可以自行选择时机释放内存，例如: Go、Java已经实现垃圾回收, C语言目前尚未实现垃圾回收，C++中可以通过智能指针达到垃圾回收的目的。 除了语言层面的内存管理外，有时我们需要在程序中自行管理内存，总体而言，对于内存管理，我认为主要是解决以下问题: 用户申请内存时，如何快速查找到满足用户需求的内存块？ 用户释放内存时，如何避免内存碎片化？ 无论是语言层面实现的内存管理还是应用程序自行实现的内存管理，大都将内存按照大小分为几种，每种采用不同的管理模式。常见的分类是按照2的整数次幂分，将不同种类的内存通过链表链接，查询时，从相应大小的链表中寻找，如果找不到，则可以考虑从更大块内存中，拿取一块，将其分为多个小点的内存。当然，对于特别大的内存，语言层面的内存管理可以直接调用内存管理相关的系统调用，应用层面的内存管理则可以直接使用语言层面的内存管理。 nginx内存管理整体可以分为2个部分， 第一部分是常规的内存池，用于进程平时所需的内存管理； 第二部分是共享内存的管理。总体而言，共享内存教内存池要复杂的多。 二. nginx内存池管理2.1 说明 本部分使用的nginx版本为1.15.3 具体源码参见src/core/ngx_palloc.c文件 2.2 nginx实现2.2.1 使用流程nginx内存池的使用较为简单,可以分为3步， 调用ngx_create_pool函数获取ngx_pool_t指针。 12//size代表ngx_pool_t一块的大小ngx_pool_t* ngx_create_pool(size_t size, ngx_log_t *log) 调用ngx_palloc申请内存使用 12//从pool中申请size大小的内存void* ngx_palloc(ngx_pool_t *pool, size_t size) 释放内存(可以释放大块内存或者释放整个内存池) 1234//释放从pool中申请的大块内存ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p)//释放整个内存池void ngx_destroy_pool(ngx_pool_t *pool) 2.2.2 具体实现 如下图所示，nginx将内存分为2种，一种是小内存，一种是大内存，当申请的空间大于pool-&gt;max时，我们认为是大内存空间，否则是小内存空间。12//创建内存池的参数size减去头部管理结构ngx_pool_t的大小pool-&gt;max = size - sizeof(ngx_pool_t); 对于小块内存空间, nginx首先查看当前内存块待分配的空间中，是否能够满足用户需求，如果可以，则直接将这部分内存返回。如果不能满足用户需求，则需要重新申请一个内存块，申请的内存块与当前块空间大小相同，将新申请的内存块通过链表链接到上一个内存块，从新的内存块中分配用户所需的内存。 小块内存并不释放，用户申请后直接使用，即使后期不再使用也不需要释放该内存。由于用户有时并不知道自己使用的内存块是大是小，此时也可以调用ngx_pfree函数释放该空间，该函数会从大空间链表中查找内存，找到则释放内存。对于小内存而言，并未做任何处理。 对于大块内存, nginx会将这些内存放到链表中存储，通过pool-&gt;large进行管理。值得注意的是，用户管理大内存的ngx_pool_large_t结构是从本内存池的小块内存中申请而来，也就意味着无法释放这些内存，nginx则是直接复用ngx_pool_large_t结构体。当用户需要申请大内存空间时，利用c函数库malloc申请空间，然后将其挂载某个ngx_pool_large_t结构体上。nginx在需要一个新的ngx_pool_large_t结构时，会首先pool-&gt;large链表的前3个元素中，查看是否有可用的,如果有则直接使用，否则新建ngx_pool_large_t结构。 三. nginx共享内存管理3.1 说明 本部分使用的nginx版本是1.15.3 本部分源码详见src/core/ngx_slab.c, src/core/ngx_shmtx.c nginx共享内存内容相对较多，本文仅做简单概述。 3.2 直接使用共享内存3.2.1 基础 nginx中需要创建互斥锁，用于后面多进程同步使用。除此之外，nginx可能需要一些统计信息，例如设置(stat_stub),对于这些变量，我们并不需要特意管理，只需要开辟共享空间后，直接使用即可。 设置stat_stub后所需的统计信息，亦是放到共享内存中，我们此处仅以nginx中的互斥锁进行说明。 3.2.2 nginx互斥锁的实现 nginx互斥锁，有两种方案，当系统支持原子操作时，采用原子操作，不支持时采用文件锁。本节源码见ngx_event_module_init函数。 下图为文件锁实现互斥锁的示意图。 下图为原子操作实现互斥锁的示意图。 问题 reload时，新启动的master向老的master发送信号后直接退出，旧的master,重新加载配置(ngx_init_cycle函数), 新创建工作进程, 新的工作进程与旧的工作进程使用的锁是相同的。 平滑升级时, 旧的master会创建新的master, 新的master会继承旧的master监听的端口(通过环境变量传递监听套接字对应的fd)，新的进程并没有重新绑定监听端口。可能存在新老worker同时监听某个端口的情况，此时操作系统会保证只会有一个进程处理该事件(虽然epoll_wait都会被唤醒)。 3.3 通过slab管理共享内存 nginx允许各个模块开辟共享空间以供使用,例如ngx_http_limit_conn_module模块。 nginx共享内存管理的基本思想有: 将内存按照页进行分配，每页的大小相同, 此处设为page_size。 将内存块按照2的整数次幂进行划分, 最小为8bit, 最大为page_size/2。例如，假设每页大小为4Kb, 则将内存分为8, 16, 32, 64, 128, 256, 512, 1024, 2048共9种，每种对应一个slot, 此时slots数组的大小n即为9。申请小块内存(申请内存大小size &lt;= page_size/2)时，直接给用户这9种中的一种，例如，需要30bit时，找大小为32的内存块提供给用户。 每个页只会划分一种类型的内存块。例如，某次申请内存时，现有内存无法满足要求，此时会使用一个新的页，则这个新页此后只会分配这种大小的内存。 通过双向链表将所有空闲的页连接。图中ngx_slab_pool_t中的free变量即使用来链接空闲页的。 通过slots数组将所有小块内存所使用的页链接起来。 对于大于等于页面大小的空间请求，计算所需页数，找到连续的空闲页，将空闲页的首页地址返回给客户使用，通过每页的管理结构ngx_slab_page_t进行标识。 所有页面只会有3中状态，空闲、未满、已满。空闲，未满都是通过双向链表进行整合，已满页面则不存在与任何页面，当空间被释放时，会将其加入到某个链表。 nginx共享内存的基本结构图如下: 在上图中，除了最右侧的ngx_slab_pool_t接口开始的一段内存位于共享内存区外，其他内存都不是共享内存。 共享内存最终是从page中分配而来。]]></content>
      <categories>
        <category>nginx</category>
        <category>内存管理</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
