<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[InnoDB Data Dictionary]]></title>
    <url>%2F2020%2F11%2F09%2Finnodb-dd%2F</url>
    <content type="text"><![CDATA[一. 基础知识 本文使用的Mysql版本: 8.0.12-debug 本文用到的Linux命令: xxd 本文需要使用到的知识点: Mysql 数据页存储, 可以参见 https://segmentfault.com/a/1190000037436803 1.1 问题 Data Dictironary是什么? Data Dictironary(DD, 数据字典)是有关数据库对象的合集, 例如表、视图、索引等, 可以看做是数据库的元信息。换句话说, 数据字典存储了有关表结构的信息, 每个表具有的列, 表的索引等。 系统表是什么? 跟自己创建的表有何不同? 系统表有很多, 常见的有mysql.schemata,mysql.tables, mysql.indexes 我们创建的表的元信息是放到系统表中的 在内存中, 这些元信息以对象的方式提供给外部使用, 比如说, 创建一个表, 内存中会创建这个表的数据字典对象, 系统表以及我们创建的表都会有自己的数据字典对象 可以认为系统表的元信息就存储在自己的数据字典对象中, 这些信息会被序列化到磁盘的mysql.ibd文件中 DD存储在哪些地方? 这里是针对Mysql 8的数据字典, 数据字典信息需要持久化, 存储在mysql.ibd文件中, 使用专门的表空间id 在每个独立的表空间中, 也备份了一份这个表空间相关的dd对象序列化信息 系统表空间, 也就是ibdata1文件中, 并没有存储dd对象的相关信息 1.2 DD存储Mysql 8之前, 数据字典存储结构如下图, 从图中可以看出, DD信息存储在多个地方, 部分数据存储在文件中 部分数据存储在mysql系统表中 InnoDB系统表也存储了部分数据 这种存储方式存在以下问题: 数据存储在多个地方, 难以维护管理 MyISAM系统表容易损坏 不支持原子操作 Mysql 8 对数据字典进行了重新设计, 并且使用InnoDB存储, 将原来存储数据字典的文件全部删除, 统一放到数据字典表空间中 对于原来使用MyISAM存储的系统表, 全部替换为InnoDB存储, 为实现原子DDL提供了可能性 1.3 如何查看数据字典 通过命令行连接Mysql服务器查看12SET SESSION debug=&apos;+d,skip_dd_table_access_check&apos;;SELECT name, schema_id, hidden, type FROM mysql.tables where schema_id=1 AND hidden=&apos;System&apos;; 可以看到DD中存储了很多系统表,12345678910111213141516+------------------------------+-----------+--------+------------+| name | schema_id | hidden | type |+------------------------------+-----------+--------+------------+| catalogs | 1 | System | BASE TABLE || character_sets | 1 | System | BASE TABLE || collations | 1 | System | BASE TABLE || columns | 1 | System | BASE TABLE || dd_properties | 1 | System | BASE TABLE || events | 1 | System | BASE TABLE || index_stats | 1 | System | BASE TABLE || indexes | 1 | System | BASE TABLE || schemata | 1 | System | BASE TABLE || tables | 1 | System | BASE TABLE || tablespace_files | 1 | System | BASE TABLE || tablespaces | 1 | System | BASE TABLE |+------------------------------+-----------+--------+------------+ 这里我们仅列出了一部分, InnoDB还存储了很多其他的系统表 这些系统表存储了各种元数据, columns存储了表的列信息, indexes则存储了表的索引信息, schemata存储了数据库的信息。 通过 ibd2sdi 工具查看1ibd2sdi test.ibd tips: 可以通过utilities/ibd2sdi.cc文件, 查看反序列化过程, 进而可以看出sdi页面存储结构 1.4 Serialized Dictionary Information(SDI) SDI是什么? SDI, 字典序列化信息, 也就是将数据字典中的对象序列化后的数据 SDI如何组织? DD中包含很多的数据字典对象, 这些对象的SDI数据以B+树的方式进行组织 SDI数据基本都是压缩后存储的, 压缩方式使用的是zlib SDI举例 1234567891011121314151617181920212223242526272829303132333435&#123; "mysqld_version_id":80012, "dd_version":80012, "sdi_version":1, "dd_object_type":"Table", "dd_object":&#123; "name":"x", "columns":[ &#123; "name":"id", "type":4, ... &#125;, &#123; "name":"DB_TRX_ID", "type":10, ... &#125;, &#123; "name":"DB_ROLL_PTR", "type":9, ... &#125; ], ... "indexes":[ &#123; "name":"PRIMARY", "hidden":false, ... &#125; ], ... &#125;&#125; 可以看到, SDI中存储了这个表的元信息, 比如说这个表的列组成, 索引信息等。 二. SDI存储页示例上文中, 我们说过在每个表的独立表空间中, 存储了这个表相关的DD对象序列化后的信息。在这一节, 我们学习如何查看SDI信息。 我们首先建立一个表1234CREATE TABLE `x` ( `id` int(11) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci 表所在的库是hbshi 查看这表的独立表空间文件, 通过 xxd x.ibd x.txt 查看这个表空间的数据页, 这里我们主要看第4页, 这1页存储了这个表相关的数据字典序列化信息, 123456789101112000c000: 723e 3b3c 0000 0003 ffff ffff ffff ffff r&gt;;&lt;............000c010: 0000 0000 0128 06fc 45bd 0000 0000 0000 .....(..E.......000c020: 0000 0000 0004 0002 04d2 8004 0000 0000 ................000c030: 0172 0001 0001 0002 0000 0000 0000 0000 .r..............000c040: 0000 ffff ffff ffff ffff 0000 0004 0000 ................000c050: 0002 00f2 0000 0004 0000 0002 0032 0100 .............2..000c060: 0201 0f69 6e66 696d 756d 0003 000b 0000 ...infimum......000c070: 7375 7072 656d 756d cb80 0000 10ff f100 supremum........000c080: 0000 0200 0000 0000 0000 0900 0000 0016 ................................000fff0: 0000 0000 0070 0063 723e 3b3c 0128 06fc .....p.cr&gt;;&lt;.(.. 对于一个表空间而言, 可能会有多个数据字典对象, 这些对象序列化后会以B+树的方式组织, 我们看一下具体的存储结构, 整体页存储结构: 最小记录: 记录头[00c05e, 00c062], 具体数据[00c063, 00c06a] 最大记录: 记录头[00c06b, 00c06f], 具体数据[00c070, 00c077] 第1条记录: 2字节的内容长度[00c078, 00c079]; 5字节的记录头[00c07a, 00c07e]; 记录内容[00c07f, 00c16a]。 第2条记录: 2字节的内容长度[00c16b, 00c16c], 5字节的记录头[00c16d, 00c171], 记录的数据[00c172, 00c4d1] 我们再看下每个记录的具体存储: 长度: 1或者2个字节的长度字段, 说明内容长度 5个字节的记录头 33个字节的记录说明 记录的具体内容, 记录头前面的1或者2个字节就是说明这个记录的内容长度的 记录说明: 4个字节的类型信息 8个字节的id 6个字节的事务id 7个字节的回滚指针 4个字节的非压缩长度 4个字节的压缩长度 存储结构如下图所示, SDI记录遍历流程 从最小记录计算第1条记录, 00c063 + 010f = 00c172 计算第2条记录, 00c172 + ff0d = 00c07f 计算第3条记录: 00c07f + fff1 = 00c070 注意: 游标以每个记录的内容开始, 例如最小记录的开始位置为00c063, 而不是00c05e。计算的下一个记录位置, 也是内容开始的位置。 如何查看SDI压缩后的数据 首先通过bin/ibd2sdi工具, 将ibd文件中的sdi信息打印 从打印中的信息中, 选择其中一项 从选中的一项中抽出object字段, 将其压缩, 这里我们使用python脚本进行压缩, 压缩方式使用zlib 12345678910111213141516171819202122#!/usr/bin/env python# -*- coding=utf-8 -*-#import osimport sysimport jsonimport timeimport datetimeimport hashlibimport zlibreload(sys)sys.setdefaultencoding(&quot;utf-8&quot;)def help(): f = open(&quot;./test.data&quot;) l = f.readline() d = zlib.compress(l) f2 = open(&quot;./tmp.data&quot;, &apos;w+&apos;) f2.write(d)help() 对比压缩后的数据以及表空间存储的数据 三. DD表空间 InnoDB常用表空间 表空间占用4个字节, 最大值为0xffff ffff, 这个表空间是无效的, 没有使用 0xffff fffe 是data dicitonary的表空间, 也就是mysql.ibd 0xffff fffd 是临时表空间, 也就是ibtmp1.ibd [0xffff fffc, 0xffff fff0]是为日志准备的 [0xffff ffef, 0xffff ff70]是undo log [0x0000 0002, 0xffff ff6f]是可以正常使用的表空间 1是 sys/sys_config 表空间 0是系统表空间, 也就是ibdata1 DD表空间存储 DD表空间存储结构与一般的独立表空间存储结构是相同的 第4页(page 3)是SDI存储的root page, SDI以B+树进行组织 除了SDI信息外, 具体的元数据表也存储在这个表空间中, 存储方式与一般的表是一样的。 四. 总结与思考 本文主要介绍了Mysql数据字典的概念以及数据字典的存储, 这里我们简单总结一下, 系统表以及我们创建的表, 元信息都存储在数据字典中 我们自己创建的表的元信息存储在系统表中, 同时也会序列化到自己的表空间中 系统表的元信息, 存储在系统表的数据字典对象中, 这些信息会被序列化到mysql.ibd文件中 SDI信息以B+树的方式进行组织 思考 DD是数据库对象的合集, Mysql server层以及InnoDB层都需要这些信息, 他们是如何具体操作的? DDL的原子性是如何实现的? 五. 参考 https://dev.mysql.com/doc/refman/8.0/en/data-dictionary-schema.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB数据页存储]]></title>
    <url>%2F2020%2F10%2F12%2FInnoDB_DataPage%2F</url>
    <content type="text"><![CDATA[一. 简介 Mysql是目前最为流行的关系型数据库管理系统, 具有体积小、速度快、开放源码等优势。InnoDB是Mysql使用最广泛的存储引擎, InnoDB进行了行锁设计, 支持MVCC, 提供一致性非锁定读。学习InnoDB数据页存储, 能够让我们更加深入的理解InnoDB的一些特性。 程序 = 数据结构 + 算法, 对于Mysql而言也是如此。由于数据持久化的需要, Mysql的数据不仅存储在内存中, 也会持久化到文件中, 存储结构如下图, 从磁盘中, 我们可以很容易的看到持久化的各个文件。 磁盘中的文件需要加载到内存中才能被程序使用, 很明显, 不可能将所有磁盘文件都加载到内存, 当内存中的数据发生更改后, 也需要刷新到磁盘文件中, 什么时候刷新, 怎么刷新, 这些都是Mysql需要考虑的问题, 但是这些内容不是本文的重点, 我们这里稍加了解即可。 本文的重点是学习数据页的存储, 这些数据页可能存在与系统表空间, 独立表空间或者临时表空间。可以看到, 这些只是图中的一小部分。 学习之前, 我们先考虑几个问题, 无论是内存存储还是磁盘存储, 都离不开内存管理, InnoDB是如何划分内存以及如何管理内存的? InnoDB使用B+树存储我们表中的数据, B+树索引节点以及叶子节点应该需要存储哪些数据? 又是怎么存储的? 我们在使用时, 创建了数据库, 数据表, 这些元数据是如何存储的, 查询某个表时, 如何根据元数据找到表的索引, 如何选择索引, 选择索引后, 如何定位到索引的根节点(root page)? 找到跟节点后, 又是如何一步步找到某个具体数据的? 说明 Mysql版本: 8.0.12-debug 存储引擎使用InnoDB 我们会用到xxd命令, 使用xxd(或者hexdump)可以以十六进制的方式查看文件。 二. InnoDB存储结构InnoDB存储结构图如下所示, 我们这里只做简要的介绍, 更多细节我们将在后续的文章中再进行详细阐述, 表空间(tablespace)可以认为是InnoDB存储引擎存储结构的最高层, 所有数据都在表空间中, 除了共享表空间外, 每个表可以创建独立表空间, 具体参数是由innodb_file_per_table参数决定, 表空间由各种段组成。 常见的段(segment)有数据段, 回滚段, 索引段。innodb中数据段就是B+树的叶子节点, 索引段就是B+树中的非叶子节点。 段是由区(extent)组成, 默认情况下区的大小是1MB, InnoDB默认页大小为16KB, 所以1个区是由16个连续页组成。 innodb默认页(page)大小是16KB, 也可以通过innodb_page_size进行控制。 innodb存储是面向行(row)的, 行的存储格式主要有compact、redundant、compressed、dynamic。 三. 数据页存储3.1 独立表空间通过innodb_file_per_table参数, 我们可以为每个表都创建一个表空间, 这个就是这个表的独立表空间, 这个表的索引段, 数据段都会存储在这个独立表空间中, 但是Redo log, Undo log仍然在各自的表空间中, 表空间存储如下图, 表空间的page 0是表空间的第一页, 存储了表空间的信息， 同时也用于管理前256个extent。page 16384类型为FIL_PAGE_TYPE_XDES也用于管理之后的256个extent, 以此类推, 每隔16384个页面都会需要一个FIL_PAGE_TYPE_XDES页面。 page 1类型是FIL_PAGE_IBUF_BITMAP, 用于管理每个page(前256个extent的16384个页面)的change buffer(change buffer相关内容不是本文的重点, 感兴趣的读者可以查找相关资料)。与FIL_PAGE_TYPE_XDES类似, 每隔16384个页面都需要一个FIL_PAGE_IBUF_BITMAP页面。 page 2类型为FIL_PAGE_INODE, 用于管理segment。 page 3类型为FIL_PAGE_SDI, 存储Serialized Dictionary Information(SDI, 词典序列化信息), 存储了这个表空间的一些数据字典(Data Dictionary)信息。 page 4一般就是这个表主键索引的root page。 3.2 页存储InnoDB的页存储结构如下, 每页都是由3部分组成, File Header(38字节)、File Body、File Trailer(8字节), 不同页的File Body存储的内容不同, File Header 名称 大小 说明 FIL_PAGE_SPACE_OR_CHKSUM 4字节 页的校验码 FIL_PAGE_OFFSET 4字节 表空间中页的便宜量 FIL_PAGE_PREV 4字节 上一页 FIL_PAGE_NEXT 4字节 下一页 FIL_PAGE_LSN 8字节 页面被最后修改时对应的日志序列位置 FIL_PAGE_TYPE 2字节 页面类型 FIL_PAGE_FILE_FLUSH_LSN 8字节 系统表空间中有定义, 代表文件更新到的LSN FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4字节 页面所属表空间id File Type 名称 值 说明 FIL_PAGE_TYPE_ALLOCATED 0x0000 未使用 FIL_PAGE_UNDO_LOG 0x0002 undo log FIL_PAGE_INODE 0x0003 存储了段信息 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A BLOB页 FIL_PAGE_SDI 0x45bd SDI索引页 FIL_PAGE_RTREE 0x45be R-tree FIL_PAGE_INDEX 0x45bf B-tree 3.3 数据页看完InnoDB页结构后, 我们看下数据页的存储, Page Header 名称 大小 说明 PAGE_N_DIR_SLOTS 2字节 page directory中slot的个数 PAGE_HEAP_TOP 2字节 堆中第一个记录指针 PAGE_N_HEAP 2字节 堆中记录数 PAGE_FREE 2字节 指向空闲空间首地址 PAGE_GARBAGE 2字节 已经删除的记录数 PAGE_LAST_INSERT 2字节 最后插入位置 PAGE_DIRECTION 2字节 最后插入方向 PAGE_N_DIRECTION 2字节 一个插入方向连续插入记录数 PAGE_N_RECS 2字节 这个页的记录总数 PAGE_MAX_TRX_ID 8字节 修改当前页的最大事务ID PAGE_LEVEL 2字节 当前页在索引中的层, 叶子节点为0x00 PAGE_INDEX_ID 8字节 索引ID PAGE_BTR_SEG_LEAF 10字节 非叶子节点所在段, 仅在B+树的root页中有定义 PAGE_BTR_SEG_TOP 10字节 数据页所在段, 仅在B+树的root页中有定义 Infimun &amp; Supermum虚拟记录, Infimum为13字节, Supermum也是13字节。具体存储内容, 我们会在下面进行介绍。 Page Directory 页目录, 因为行记录在数据页中以链表的形式链接, 但是在查找记录时, 链表查找速度很慢, 为了加速记录查找, 创建页目录, 页目录可以用于二分查找。每个目录项占用2个字节, 从页尾部开始, 倒序存储。 为了便于理解Page Directory, 我们这里举一个例子, 如果表中存储了200条数据, 数据通过链表的方式进行链接, 我们在查询时, 需要遍历整个链表才能找到数据, 这样无疑比较慢。我们可以通过建立索引的方式, 加快查找速度, 我们可以将这200条记录的主键按照顺序进行存储, InnoDB的Page Directory就是这个思路, 但是并不是存储了主键的值, 而是存储了对应记录的位置, 并且不是将每个行记录都存储在Page Directory中, 只是建立一个稀疏索引。 3.4 innodb行存储限于篇幅, 我们这里主要介绍compact格式的行记录存储, 存储格式如下图, 从图中可以看出, 每个记录行至少占有5字节(记录头) + 主键长度 + 6字节(事务ID) + 7字节(回滚指针) 我们需要注意记录头中的next_record字段, 这个字段占有16bit, 也就是2个字节, 通过这个字段, InnoDB将一个页中的所有记录以链表的方式链接到一起。 四. 实例讲解为了便于大家理解, 这部分我们给出一些实例, 本节举例说明InnoDB的一个表是如何存储的, 主要介绍两种情况, 一种情况是表中数据很少, 另一种情况是表中数据比较多, 一页已经存储不了的情况。 表结构定义,1create table `t` (`id` int not null, primary key(`id`)) engine=InnoDB ROW_FORMAT=Compact; 为了更容易理解, 我们这里只创建了一个非常简单的表, 也只有一个主键索引。主键类型为int, 占用4个字节。 创建表后, 可以在相应的目录下看到t.ibd文件, 这里我是在test数据库下创建的这个表, 所以也就在test目录下。 从磁盘文件中, 我们可以看到, t.ibd文件大小为112KB, 也就是7*16KB, 也就是7个page, 也就意味着, 创建表后, InnoDB默认初始化了7个page。 我们的表中没有变长字段, 主键长度为4字节, 所以单个记录的长度为5(记录头) + 4(主键ID) + 6(事务ID) + 7(回滚指针) = 22字节 B+树示例InnoDB数据存储是通过B+树组织的, 一个很简单的B+树如下所示, B+树的性质有很多, 其增删查改操作较常规的二叉树更复杂一些, 感兴趣的可以查询相关资料, 这里有个基本概念即可。 后续如果没有特殊说明, 表空间第一个页是page 0, 第二页是page 1, 以此类推。 4.1 单页存储我们首先看下当表中数据很少的时候, 数据是如何组织的, 具体操作步骤如下, 我们向表中插入2条记录,12insert into t values (2);insert into t values (1); 这里注意我们先插入主键值为2的行记录, 再插入主键值为1的行记录。 通过xxd将t.ibd以16进制表示, 执行命令xxd t.idb t.txt, 也可以使用hexdump命令查看。 查看t.txt中的内容, 这里我们查看page 4的数据 123456789101112131415160010000: a76e 6043 0000 0004 ffff ffff ffff ffff .n`C............0010010: 0000 0000 012e 6d9d 45bf 0000 0000 0000 ......m.E.......0010020: 0000 0000 0005 0002 00a4 8004 0000 0000 ................0010030: 0093 0001 0001 0002 0000 0000 0000 0000 ................0010040: 0000 0000 0000 0000 0091 0000 0005 0000 ................0010050: 0002 0272 0000 0005 0000 0002 01b2 0100 ...r............0010060: 0200 3069 6e66 696d 756d 0003 000b 0000 ..0infimum......0010070: 7375 7072 656d 756d 0000 10ff f380 0000 supremum........0010080: 0200 0000 001c 0481 0000 00fa 0110 0000 ................0010090: 18ff ea80 0000 0100 0000 001c 0582 0000 ................00100a0: 012c 0110 0000 0000 0000 0000 0000 0000 .,..............00100b0: 0000 0000 0000 0000 0000 0000 0000 0000 ............................0013fe0: 0000 0000 0000 0000 0000 0000 0000 0000 ................0013ff0: 0000 0000 0070 0063 a76e 6043 012e 6d9d .....p.c.n`C..m. 前38字节是文件头[0010000,0010026] 之后56字节是数据页头部[0010027,001005d] 之后的26字节是最小记录[001005e, 001006a], 最大记录[001006b, 0010077], 这里可以看到最小记录的n_owns值为1(只有自身1条记录), 最大记录的n_owns值为3(除了自身外, 还有我们插入的两条记录) 紧接着是第1条插入记录[0010078, 001008d] 120010070: .... .... .... .... 0000 10ff f380 00000010080: 0200 0000 001c 0481 0000 00fa 0110 .... 最后是刚才插入的第2条记录[001008e, 00100a3] 对于int类型, innodb存储方式与常规的方式不同, [0x00000000, 0x7fffffff]代表[-2147483648, -1], [0x80000000, 0xffffffff]代表[0, 21473647]。 存储结构如下图 这里示例下如何从最小记录查找到最大记录 首先定位到最小记录的位置, 最小记录占有5字节(记录头) + 8字节(内容) = 13字节, 最小记录所在的位置为0010063, 根据最小记录的记录头信息, 可以计算出下一个记录所在位置0010063 + 0030 = 0010093。这里需要注意的是, 记录所在位置使用的是内容开始的位置。 0010093是主键为1的记录所在位置, 接着计算下一个记录的位置0010093 + ffea = 001007d, 这里需要注意的是, 加法运算时, 只保留后面4位的结果, 可以看到这个位置就是我们第一次插入的主键为2的记录 之后, 继续计算下一个记录所在位置, 001007d + fff3 = 10070, 这个就是最大记录所在的位置 在查找某个具体的行记录时, 可以先利用page directory进行近似的二分查找, 之后再进行链表查找。 page directory 页尾部包含两个slots 10013ff0: 0000 0000 0070 0063 .... .... .... .... 0063是第1个slot的位置, 相应的记录所在位置为0010063, 也就是最小记录。[001005e, 0010062]这个是最小记录的记录头, [0010063, 001006a]是最小记录的内容。 0070是第2个slot的位置, 相应的记录所在位置为0010070, 这个是最大记录所在的内容开始位置。 小结 可以看到, 从最小记录开始, 到最大记录结束, 数据按照主键顺序以链表的方式进行链接。 行数据的存储是按照插入的顺序存储的, 不是按照主键顺序存储, 数据删除后, 释放的空间可以复用, 关于复用部分的细节, 后续文章再进行详细介绍。 4.2 多页存储在4.1的基础上, 我们继续插入数据, 操作步骤如下, 我们通过脚本向表中继续插入数据12345678910111213141516171819&lt;?php$servername = "localhost:8083";$username = "root";$password = "password";$dbname = "test";try &#123; $conn = new PDO("mysql:host=$servername;dbname=$dbname", $username, $password); $conn-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION); for($i = 3; $i &lt; 1000; $i++)&#123; $sql = "INSERT INTO t VALUES (" . strval($i) . ")"; $conn-&gt;exec($sql); &#125;&#125;catch(PDOException $e)&#123; echo $sql . "&lt;br&gt;" . $e-&gt;getMessage();&#125;$conn = null;?&gt; 之前表中已经插入2条记录, 这里又插入997条记录, 所以表中现在一共999条记录, 主键id从1到999。 单条记录需要占用22字节, 可以知道, 此时, 单个数据页不能存储全部数据。 以16进制查看此时的t.ibd文件: xxd t.ibd t.txt 查看t.txt内容, 首先查看page 4的内容12345678910111213140010000: df67 193d 0000 0004 ffff ffff ffff ffff .g.=............0010010: 0000 0000 0132 5500 45bf 0000 0000 0000 .....2U.E.......0010020: 0000 0000 0005 0002 0092 8004 0000 0000 ................0010030: 008a 0002 0001 0002 0000 0000 0000 0000 ................0010040: 0001 0000 0000 0000 0091 0000 0005 0000 ................0010050: 0002 0272 0000 0005 0000 0002 01b2 0100 ...r............0010060: 0200 1a69 6e66 696d 756d 0003 000b 0000 ...infimum......0010070: 7375 7072 656d 756d 1000 1100 0d80 0000 supremum........0010080: 0100 0000 0500 0019 ffe6 8000 0153 0000 .............S..0010090: 0006 0000 0000 0000 0000 0000 0000 0000 ..............................0013fe0: 0000 0000 0000 0000 0000 0000 0000 0000 ................0013ff0: 0000 0000 0070 0063 df67 193d 0132 5500 .....p.c.g.=.2U. 可以看到, 第5页, 目前只有少量内容, 因为此时第5页是索引页, 是B+树的根, 没有存储具体的数据, 只存储了主键索引。 File Header, Page Header, Infimum &amp; Supremum跟之前基本类似, 这里就不再详细介绍。 单个索引需要占用5字节(记录头) + 4字节(主键) + 4字节(记录所在页) = 13字节。 第1个索引信息120010070: .... .... .... .... 1000 1100 0d80 00000010080: 0100 0000 05.. .... .... .... .... .... 主键id为0x80000001, 也就是1, page no为0x00000005, 也就是page 5 第2个索引信息120010080: .... .... ..00 0019 ffe6 8000 0153 00000010090: 0006 .... .... .... .... .... .... .... 主键id为0x80000153, 也就是339, page no为0x00000006, 也就是page 6 通过这两个索引信息, 可以知道, page 5存储着主键id从1到338的数据, page 6存储着主键id从339到999的数据 查看page 51234567891011121314151617181920212223242526272829303132333435360014000: e1c0 bb7a 0000 0005 ffff ffff 0000 0006 ...z............0014010: 0000 0000 0132 5500 45bf 0000 0000 0000 .....2U.E.......0014020: 0000 0000 0005 0056 3a90 82a6 1d89 1d0c .......V:.......0014030: 0000 0005 0000 0152 0000 0000 0000 0000 .......R........0014040: 0000 0000 0000 0000 0091 0000 0000 0000 ................0014050: 0000 0000 0000 0000 0000 0000 0000 0100 ................0014060: 0200 1a69 6e66 696d 756d 0003 000b 0000 ...infimum......0014070: 7375 7072 656d 756d 0000 1000 1680 0000 supremum........0014080: 0100 0000 001c 0582 0000 012c 0110 0000 ...........,....0014090: 1800 1680 0000 0200 0000 001c 0481 0000 ................00140a0: 00fa 0110 0000 2000 1680 0000 0300 0000 ...... .......................0017e90: 0000 0000 0000 0000 0000 0000 0000 0000 ................0017ea0: 0000 0000 0070 3a27 39cf 3977 391f 38c7 .....p:&apos;9.9w9.8.0017eb0: 386f 3817 37bf 3767 370f 36b7 365f 3607 8o8.7.7g7.6.6_6.0017ec0: 35af 3557 34ff 34a7 344f 33f7 339f 3347 5.5W4.4.4O3.3.3G0017ed0: 32ef 3297 323f 31e7 318f 3137 30df 3087 2.2.2?1.1.170.0.0017ee0: 302f 2fd7 2f7f 2f27 2ecf 2e77 2e1f 2dc7 0//././&apos;...w..-.0017ef0: 2d6f 2d17 2cbf 2c67 2c0f 2bb7 2b5f 2b07 -o-.,.,g,.+.+_+.0017f00: 2aaf 2a57 29ff 29a7 294f 28f7 289f 2847 *.*W).).)O(.(.(G0017f10: 27ef 2797 273f 26e7 268f 2637 25df 2587 &apos;.&apos;.&apos;?&amp;.&amp;.&amp;7%.%.0017f20: 252f 24d7 247f 2427 23cf 2377 231f 22c7 %/$.$.$&apos;#.#w#.&quot;.0017f30: 226f 2217 21bf 2167 210f 20b7 205f 2007 &quot;o&quot;.!.!g!. . _ .0017f40: 1faf 1f57 1eff 1ea7 1e4f 1df7 0070 1d47 ...W.....O...p.G0017f50: 1cef 1c97 1c3f 1be7 1b8f 1b37 1adf 1a87 .....?.....7....0017f60: 1a2f 19d7 197f 1927 18cf 1877 181f 17c7 ./.....&apos;...w....0017f70: 176f 1717 16bf 1667 160f 15b7 155f 1507 .o.....g....._..0017f80: 14af 1457 13ff 13a7 134f 12f7 129f 1247 ...W.....O.....G0017f90: 11ef 1197 113f 10e7 108f 1037 0fdf 0f87 .....?.....7....0017fa0: 0f2f 0ed7 0e7f 0e27 0dcf 0d77 0d1f 0cc7 ./.....&apos;...w....0017fb0: 0c6f 0c17 0bbf 0b67 0b0f 0ab7 0a5f 0a07 .o.....g....._..0017fc0: 09af 0957 08ff 08a7 084f 07f7 079f 0747 ...W.....O.....G0017fd0: 06ef 0697 063f 05e7 058f 0537 04df 0487 .....?.....7....0017fe0: 042f 03d7 037f 0327 02cf 0277 021f 01c7 ./.....&apos;...w....0017ff0: 016f 0117 00bf 0063 e1c0 bb7a 0132 5500 .o.....c...z.2U. 注意页尾部包含page directory, slots的个数可以从page header中读取 File Header中的FIL_PAGE_NEXT字段, 值为0x00000006, 也就是page no为6的页。 查看page 61234567891011121314151617181920212223242526272829303132330018000: 2ddb 788c 0000 0006 0000 0005 ffff ffff -.x.............0018010: 0000 0000 0133 f431 45bf 0000 0000 0000 .....3.1E.......0018020: 0000 0000 0005 00a6 3946 8297 0000 0000 ........9F......0018030: 3935 0002 0142 0295 0000 0000 0000 0000 95...B..........0018040: 0000 0000 0000 0000 0091 0000 0000 0000 ................0018050: 0000 0000 0000 0000 0000 0000 0000 0100 ................0018060: 0200 1a69 6e66 696d 756d 0006 000b 0000 ...infimum......0018070: 7375 7072 656d 756d 0000 1000 1680 0001 supremum........0018080: 5300 0000 001d 6d81 0000 00a3 0110 0000 S.....m.......................001bea0: 0000 0000 0000 0000 0000 0000 0070 38c7 .............p8.001beb0: 386f 3817 37bf 3767 370f 36b7 365f 3607 8o8.7.7g7.6.6_6.001bec0: 35af 3557 34ff 34a7 344f 33f7 339f 3347 5.5W4.4.4O3.3.3G001bed0: 32ef 3297 323f 31e7 318f 3137 30df 3087 2.2.2?1.1.170.0.001bee0: 302f 2fd7 2f7f 2f27 2ecf 2e77 2e1f 2dc7 0//././&apos;...w..-.001bef0: 2d6f 2d17 2cbf 2c67 2c0f 2bb7 2b5f 2b07 -o-.,.,g,.+.+_+.001bf00: 2aaf 2a57 29ff 29a7 294f 28f7 289f 2847 *.*W).).)O(.(.(G001bf10: 27ef 2797 273f 26e7 268f 2637 25df 2587 &apos;.&apos;.&apos;?&amp;.&amp;.&amp;7%.%.001bf20: 252f 24d7 247f 2427 23cf 2377 231f 22c7 %/$.$.$&apos;#.#w#.&quot;.001bf30: 226f 2217 21bf 2167 210f 20b7 205f 2007 &quot;o&quot;.!.!g!. . _ .001bf40: 1faf 1f57 1eff 1ea7 1e4f 1df7 1d9f 1d47 ...W.....O.....G001bf50: 1cef 1c97 1c3f 1be7 1b8f 1b37 1adf 1a87 .....?.....7....001bf60: 1a2f 19d7 197f 1927 18cf 1877 181f 17c7 ./.....&apos;...w....001bf70: 176f 1717 16bf 1667 160f 15b7 155f 1507 .o.....g....._..001bf80: 14af 1457 13ff 13a7 134f 12f7 129f 1247 ...W.....O.....G001bf90: 11ef 1197 113f 10e7 108f 1037 0fdf 0f87 .....?.....7....001bfa0: 0f2f 0ed7 0e7f 0e27 0dcf 0d77 0d1f 0cc7 ./.....&apos;...w....001bfb0: 0c6f 0c17 0bbf 0b67 0b0f 0ab7 0a5f 0a07 .o.....g....._..001bfc0: 09af 0957 08ff 08a7 084f 07f7 079f 0747 ...W.....O.....G001bfd0: 06ef 0697 063f 05e7 058f 0537 04df 0487 .....?.....7....001bfe0: 042f 03d7 037f 0327 02cf 0277 021f 01c7 ./.....&apos;...w....001bff0: 016f 0117 00bf 0063 2ddb 788c 0133 f431 .o.....c-.x..3.1 注意File Header中的FIL_PAGE_PREV字段, 值为0x00000005, 也就是page no为5的页。 结合page 5可以看出, 叶子节点的两个页通过链表进行链接, 每个页内的数据通过记录头中的next_record字段进行链接。 存储结构图如下, 小结 对于单页存储不了的情况, 需要进行页分裂, 此时B+树会有多层结构, 最低层为叶子节点, 存储了具体的数据, 上面是索引节点, 只存储主键以及下一层节点所在的页信息 五. 总结与思考本文介绍了innodb的数据页存储, 以实例的方式讲解了innodb存储引擎如何存储一个表中数据的。但是我们仍然有很多问题没有给出答案, 查找行记录时, 需要找到某个索引的root page, 这个信息是存储在哪里的? 我们没有介绍段和区的相关内容, 这些在InnoDB数据存储时是如何使用的? 我们查看数据时, 都是直接查看磁盘文件, 内存中的页与磁盘中的页有何区别, 内存中的脏页又是如何刷新到磁盘的? InnoDB存储引擎较为复杂, 不可能一次性将全部内容学会, 我们不妨每次带入一个问题, 深入寻找这个问题的答案, 关于这些问题, 我会在后续文章中再逐步介绍。 六. 参考 &lt;&lt;Mysql技术内幕 InnoDB存储引擎&gt;&gt; 淘宝数据库内核月报]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈C/C++链接库]]></title>
    <url>%2F2019%2F08%2F28%2Flink-library-introduce%2F</url>
    <content type="text"><![CDATA[一. 说明 本文后续代码的编译以及执行环境为Centos 7.6 x86_64, g++ 4.8.5 本文后续会用到linux下nm, ldd命令。nm用于查看文件中的符号, 例如变量, 函数名称。ldd用于查看动态链接库或者可执行文件的依赖库(动态链接库)。 二. 编译链接 程序员写出的代码为.c或者.cpp, 这些文件需要经过: 预处理(处理代码中的include, 宏等)、编译(生成汇编代码)、汇编(将汇编代码生成二进制文件)、链接才能生成可执行程序。本文将预处理、编译、汇编的过程都看做是编译, 简化读者理解。更多细节可以参考相关资料。 生成可执行文件后, 通过终端进行执行 g++参数说明, -std=c++11: 使用c++11标准 -o: 指定输出文件名称 链接器ld参数: -L: 指定链接时搜索的动态链接库路径 -l: 链接某个库, 例如链接libmath.so, 写为-lmath 2.1 编译 对于c或者c++项目而言, 我们认为单个c或者cpp文件是一个编译单元, 通过编译器(gcc, g++, clang, clang++)可以生成编译后的二进制文件。例如: 编译file1.cpp, 可以生成file1.o。对于单个编译单元而言, 里面会有一些符号, 例如函数名称, 变量名称, 类名。这些符号可以分为三类: 对外提供的, 也就是说其他的编译单元可以使用的 对外依赖的, 也就是说本单元需要外部的其他编译单元提供的符号 自己内部使用的, 这种符号只有本编译单元自身需要使用, 外部不可见 通过nm, 我们可以查看某个编译单元存在哪些符号 2.2 链接 C/C++项目中含有很多个c文件或者cpp文件, 这些文件经过编译生成了对应的二进制文件。需要通过链接器将这些文件链接, 进而生成可执行程序。 linux下链接器为ld, 利用该工具我们可以将这些文件链接, 进而生成可执行程序。 在进行链接时, 每个编译单元需要的符号, 都需要能够找到对应的定义。例如: 某个编译单元需要其他编译单元提供符号fun1, 这是一个函数, 如果链接器没能从其他编译单元找到这个符号, 就会报我们经常看到的未定义错误。若果出现多次, 则会报出重复定义的错误。 2.3 示例 math.h 123456#ifndef _MATH_H_#define _MATH_H_int add(int a, int b);#endif math.cpp 12345#include "math.h"int add(int a, int b)&#123; return a + b;&#125; main.cpp 1234567891011#include &lt;iostream&gt;#include "math.h"using namespace std;int main(int argc, char **argv)&#123; int a = 100, b = 200; int result = add(a, b); cout &lt;&lt; result &lt;&lt; endl;&#125; 生成可执行文件 编译math.cpp: g++ -std=c++11 -c math.cpp, 生成math.o 编译main.cpp: g++ -std=c++11 -c main.cpp, 生成main.o 生成可以执行的文件: g++ -v math.o main.o -o main, 可以看到g++的编译链接过程 1234567891011Using built-in specs.COLLECT_GCC=g++COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapperTarget: x86_64-redhat-linuxConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linuxThread model: posixgcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)COMPILER_PATH=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/:/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/:/usr/libexec/gcc/x86_64-redhat-linux/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/:/usr/lib/gcc/x86_64-redhat-linux/LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/4.8.5/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/:/lib/../lib64/:/usr/lib/../lib64/:/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../:/lib/:/usr/lib/COLLECT_GCC_OPTIONS=&apos;-v&apos; &apos;-o&apos; &apos;main&apos; &apos;-shared-libgcc&apos; &apos;-mtune=generic&apos; &apos;-march=x86-64&apos;/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/collect2 --build-id --no-add-needed --eh-frame-hdr --hash-style=gnu -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 -o main /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/crt1.o /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/crti.o /usr/lib/gcc/x86_64-redhat-linux/4.8.5/crtbegin.o -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64 -L/lib/../lib64 -L/usr/lib/../lib64 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../.. main.o math.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-redhat-linux/4.8.5/crtend.o /usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/crtn.o 其中最后一行调用collect2(对ld进行了包装)会执行真正的链接操作, 我们直接调用这一句也可以生成main可执行文件 可以看出linux下的链接操作比较复杂, 不是简单的ld main.o math.o即可成功的。 三. 问题通过上面的介绍, 我们知道一个c/cpp文件通过编译链接, 最终生成可执行文件。无论任何语言, 程序员在写代码时, 都不可避免需要使用到库, 本文主要介绍C/C++中的库, 总体而言, 我们将这些库分为静态链接库(通常以.a结尾)，动态链接库(通常以.so结尾)。首先我们来看几个问题: 什么是静态链接库?什么是动态链接库? 静态链接库如何生成?动态链接库如何生成? 静态链接库是否可以依赖其他的静态链接库? 是否可以依赖其他动态链接库? 动态链接库是否可以依赖其他的静态链接库? 是否可以依赖其他的动态链接库? 链接静态库时?其依赖的库该如何链接? 链接动态库时?其依赖的库该如何链接? 使用第三方库时, 使用静态链接库还是动态链接库? 四. Hello World本节以hello world为例, 123456#include &lt;iostream&gt;using namespace std;int main(int argc, char **argv)&#123; cout &lt;&lt; "hello world" &lt;&lt; endl;&#125; 编译程序: g++ -std=c++11 -o main main.cpp 使用ldd查看main的依赖: ldd main 123456linux-vdso.so.1 =&gt; (0x00007ffcf53fa000)libstdc++.so.6 =&gt; /lib64/libstdc++.so.6 (0x00007f7828b3b000)libm.so.6 =&gt; /lib64/libm.so.6 (0x00007f7828839000)libgcc_s.so.1 =&gt; /lib64/libgcc_s.so.1 (0x00007f7828623000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f7828256000)/lib64/ld-linux-x86-64.so.2 (0x00007f7828e42000) 可以看出, 最简单的hello world程序也需要链接一些库 上述的几种链接库, 感兴趣的可以逐个研究 五. 动态链接库 vs 静态链接库 本节以2.3中的示例代码为例, 将math.h, math.cpp打包为静态链接库以及动态链接库, 在main.cpp中引用 5.1 静态链接库 编译: g++ -std=c++11 -fPIC -c math.cpp fPIC用于生成位置无关的代码, 更多细节可以查找相关资料 生成静态链接库: ar -crv libmath.a math.o 使用这个静态链接库: 使用静态库时, 我们需要math.h文件, 这个文件中定义了这个库对外提供的功能 除了math.h文件, 我们需要在链接阶段链接libmath.a 示例: main.cpp中已经导入了math.h文件, 编译main.c并链接libmath.a, g++ -std=c++11 -o main main.cpp -L. -lmath ldd main可以看出, main文件不再依赖libmath.a文件 5.2 动态链接库 生成动态链接库: g++ -std=c++11 -shared -fPIC math.cpp -o libmath.so 使用动态链接库: 需要使用math.h头文件, 该文件定义了库对外提供的功能 链接阶段需要链接libmath.so 示例: g++ -std=c++11 -o main main.cpp -L. -lmath 执行main, 会发现无法执行 1./main: error while loading shared libraries: libmath.so: cannot open shared object file: No such file or directory 我们先用ldd 查看main的依赖库: 1234567linux-vdso.so.1 =&gt; (0x00007ffd2adde000)libmath.so =&gt; not foundlibstdc++.so.6 =&gt; /lib64/libstdc++.so.6 (0x00007fd3b7ee6000)libm.so.6 =&gt; /lib64/libm.so.6 (0x00007fd3b7be4000)libgcc_s.so.1 =&gt; /lib64/libgcc_s.so.1 (0x00007fd3b79ce000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fd3b7601000)/lib64/ld-linux-x86-64.so.2 (0x00007fd3b81ed000) 很奇怪, libmath.so没有找到, 我们在第三步编译时明明将这个库加入进去了。这个是由于, 在链接阶段, 链接器可以在当前目录找到libmath.so。执行阶段, 搜索动态链接库时, 并没有包含当前目录, 所以报错。我们可以通过export LD_LIBRARY_PATH=/libpath将libmath.so所在路径放入动态链接库的搜索路径中。此时即可成功执行。 5.3 对比 静态链接库, 动态链接库都是二进制文件(ELF格式, 详细信息可以查找相关资料) 从静态链接库生成的过程来看, 其本质就是将多个编译单元(.o文件), 打包为一个新的文件。链接静态链接库时, 会将静态链接库的代码合并进程序中。 链接动态链接库时, 并不会将动态链接库的内容合并进代码中, 而是在程序执行时, 搜索动态链接库, 再进行链接。 六. 库之间的依赖6.1 源代码 first.h 12345678#ifndef __FIRST_H_#define __FIRST_H_#include &lt;cstdio&gt;void first();#endif first.cpp 123456#include"first.h"void first()&#123; printf("This is first!\n");&#125; second.h 1234567#ifndef __SECOND_H_#define __SECOND_H_ #include &lt;cstdio&gt;void second();#endif second.cpp 12345678#include"first.h"#include"second.h"void second()&#123; printf("This is second!\n"); first();&#125; main.cpp 123456#include&quot;second.h&quot;int main()&#123; second(); return 0;&#125; 6.2 静态库依赖静态库 生成libfirst.a静态链接库 12g++ -std=c++11 -fPIC -c first.cppar -crv libfirst.a first.o 生成libsecond.a并链接libfirst.a 12g++ -std=c++11 -c second.cpp -L. -lfirstar -crv libsecond.a second.o main.cpp中使用libsecond.a执行: g++ -std=c++11 main.cpp -L. -lsecond -o main会出现以下错误:./libsecond.a(second.o): In function second()&#39;: second.cpp:(.text+0xf): undefined reference tofirst()’collect2: error: ld returned 1 exit status 解释说明 通过nm, 我们查看libsecond.a中的符号, 找出未定义的符号, 执行nm -u libsecond.a, 即可发现first并没有定义(编译器编译后的符号并不是first, 我这里是_Z5firstv)。我们明明在生成libsecond.a时链接了libfirst.a? 主要的原因是: 生成静态链接库时, 只是将second.cpp生成的second.o打包, 并没有真正的将libfirst.a中的内容链接进libsecond.a 静态库不与其他静态库链接。我们使用archiver工具(例如Linux上的ar)将多个静态链接库打包为一个静态链接库 解决方案 将first.cpp, second.cpp打包为一个静态链接库: g++ -std=c++11 -fPIC -c first.cpp second.cpp, ar -crv libsecond.a first.o second.o。main中可以直接链接libsecond.a即可 同时链接libsecond.a, libfirst.a 6.3 动态库依赖静态库 生成libfirst.a静态链接库, 这一步与5.2节相同 生成libsecond.so静态链接libfirst.a1g++ -std=c++11 second.cpp -fPIC -shared -o libsecond.so -L. -lfirst nm -u libseond.so, 我们可以看出, 并没有出现first, 也就是说, libfirst.a已经被链接进libsecond.so中了 编译main.cpp1g++ -std=c++11 main.cpp -L. -lsecond -o main 6.4 静态库依赖动态库 生成libfirst.so 1g++ -std=c++11 first.cpp -shared -fPIC -o libfirst.so 生成libsecond.a链接libfirst.so 12g++ -std=c++11 -c second.cpp -fPIC -L. -lfirstar crv libsecond.a second.o nm -u libsecond.a, 可以看到_Z5firstv, 说明并没有将libfirst.so中包含进libsecond.a 编译main.cpp1g++ -std=c++11 main.cpp -L. -lsecond -lfirst -o main 如果没有链接first, 会发现链接错误, 找不到first函数的定义 6.5 动态库依赖动态库 生成libfirst.so 1g++ -std=c++11 first.cpp -shared -fPIC -o libfirst.so 生成libsecond.so链接libfirst.so 1g++ -std=c++11 second.cpp -shared -fPIC -o libsecond.so -L. -lfirst nm -u libsecond.so, 可以看到_Z5firstv, 这个就是first函数 ldd libsecond.so, 也可以看到libfirst.so 可以看出, 使用libsecond.so时, 仍然需要libfirst.so 编译main.cpp1g++ -std=c++11 main.cpp -L. -lsecond -o main 可以看出, 能够成功编译。 之前讲过libsecond.so需要依赖libfirst.so, 此处为何我们只链接libsecond.so也能成功呢?这里是因为链接器会自动搜索动态链接库的依赖库 七. 总结 c或者cpp文件经过编译、链接生成可执行文件 单个c文件或者cpp文件是一个编译单元。每个编译单元存在3种符号: 自己使用的, 依赖于外部的以及对外提供的。 链接器是将多个编译单元的符号相互链接以形成可执行文件。 库可以分为静态链接库(.a)以及动态链接库(.so)。 使用库时, 除了库文件, 还需要对应的头文件。 单个c文件或者cpp文件, 可能依赖其他的库文件, 但是在编译时, 只需要有声明, 并不需要有具体的定义。 静态库没有链接操作, 静态库只是将多个.o文件打包, 并没有其他操作。静态库可能依赖其他的静态库或者其他的动态库, 用户在使用静态库时, 需要手动链接这些依赖。 动态库有链接操作, 创建动态库时可以链接其他的库, 也可以不链接, 如果链接静态库, 则会将静态库的内容全部放入动态库, 如果链接动态库, 只是放入符号, 在程序初始化时, 将依赖的这些动态库也加载。如果这个动态库依赖了其他库, 但是没有链接, 也可以生成动态库, 但用户在使用这个动态链接库时, 需要手动链接这些依赖, 由于使用者很难知道这些依赖, 所以通常不使用这种方式。 总体而言, 动态库在程序执行阶段才会装进程序, 静态库则在链接阶段直接放进程序。动态库可以由多个程序共享, 节省内存，易于升级。静态库外部依赖少, 更易于部署。 八. 扩展 动态库升级问题?假设现在有2个程序: p1, p2, 一个动态链接库libmath.so.1。如果现在math库提供了新版本libmath.so.2, 程序p1需要使用libmath.so.2的新功能, p2则不想使用, 此时该如何升级math库? 如果math不兼容前一版, 则系统中需要同时存在两个版本的math库, p1, p2分别链接不同的版本 如果math兼容前一版, 系统中是否可以只保留新版的math库呢?此时p1, p2又是否需要重新编译呢?这个问题留给读者自行思考。 某个动态链接库lib1动态链接了库libbase, 现在应用程序中使用了lib1以及libbase, 编译应用程序时, 是否需要链接libbase? 应用程序不仅需要链接lib1, 也需要链接libbase 链接lib1只能保证应用程序依赖lib1的部分能够正确解析 虽然lib1动态链接了libbase, 但是动态链接真正进行符号解析是在程序执行阶段, 编译阶段无法获取libbase的相关信息, 应用程序中如果也使用了libbase中的函数, 则必须链接libbase, 否则会出现符号未定义 如果lib1静态链接了libbase, 也就是说包含了libbase中的函数, 则应用程序不需要在链接libbase 菱形依赖问题, A依赖于B以及C, B、C都依赖于D, 但是是不同版本, 例如B依赖于D1, C依赖于D2, 这种情况下如何链接? D2兼容于D1(ABI层面兼容), 程序直接链接D2 D2不兼容于D1, 查看B是否可以依赖D2重新编译 链接器的参数, 直接链接两个版本。ld的参数–default-symver或者–version-script 讨论 动态链接会有大量的依赖问题(windows dll hell) 由于采用模块化, 又允许升级单个模块, 菱形依赖问题对于很多语言都是存在的 rust, go等语言都开始采用源码编译的方式, 解决依赖问题 九. 参考 http://blog.chinaunix.net/uid-26548237-id-3837099.html https://www.cnblogs.com/fnlingnzb-learner/p/8119729.html https://blog.csdn.net/coolwaterld/article/details/85088288 https://blog.habets.se/2012/05/Shared-libraries-diamond-problem.html]]></content>
      <categories>
        <category>C/C++</category>
        <category>链接库</category>
      </categories>
      <tags>
        <tag>编译</tag>
        <tag>链接</tag>
        <tag>动态链接库</tag>
        <tag>静态链接库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swoole_server_introduce]]></title>
    <url>%2F2019%2F05%2F27%2Fswoole-server-introduce%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 SwooleSwoole是面向生产环境的php异步网络通信引擎, php开发人员可以利用Swoole开发出高性能的server服务。Swoole的server部分, 内容很多, 也涉及很多的知识点, 本文仅对其server进行简单的概述, 具体的实现细节在后续的文章中再进行详细介绍。 1.2 网络编程 网络通信是指在一台(或者多台)机器上启动一个(或者多个)进程, 监听一个(或者多个)端口, 按照某种协议(可以是标准协议http, dns; 也可以是自行定义的协议)与客户端交换信息。 目前的网络编程多是在tcp, udp或者更上层的协议之上进行编程。Swoole的server部分是基于tcp以及udp协议的。 利用udp进行编程较为简单, 本文主要介绍tcp协议之上的网络编程 TCP网络编程主要涉及4种事件, 连接建立: 主要是指客户端发起连接(connect)以及服务端接受连接(accept) 消息到达: 服务端接受到客户端发送的数据, 该事件是TCP网络编程最重要的事件, 服务端对于该类事件进行处理时, 可以采用阻塞式或者非阻塞式, 除此之外, 服务端还需要考虑分包, 应用层缓冲区等问题 消息发送成功: 发送成功是指应用层将数据成功发送到内核的套接字发送缓冲区中, 并不是指客户端成功接受数据。对于低流量的服务而言, 数据通常一次性即可发送完, 并不需要关心此类事件。如果一次性不能将全部数据发送到内核缓冲区, 则需要关心消息是否成功发送(阻塞式编程在系统调用(write, writev, send等)返回后即是发送成功, 非阻塞式编程则需要考虑实际写入的数据是否与预期一致) 连接断开: 需要考虑客户端断开连接(read返回0)以及服务端断开连接(close, shutdown) tcp建立连接的过程如下图, 图中, ACK、SYN表示标志位, seq、ack为tcp包的序号以及确认序号 tcp断开连接的过程如下图, 上图考虑的是客户端主动断开连接的情况, 服务端主动断开连接也类似 图中, FIN、ACK表示标志位, seq、ack为tcp包的序号以及确认序号 1.3 进程间通信 进程之间的通信有无名管道(pipe), 有名管道(fifo), 信号(signal), 信号量(semaphore), 套接字(socket), 共享内存(shared memory)等方式 Swoole中采用unix域套接字(套接字的一种)用于多进程之间的通信(指Swoole内部进程之间) 1.4 socketpair socketpair用于创建一个套接字对, 类似于pipe, 不同的是pipe是单向通信, 双向通信需要创建两次, socketpair调用一次即可实现双向通信, 除此之外, 由于使用的是套接字, 还可以定义数据交换的方式 socketpair系统调用123456int socketpair(int domain, int type, int protocol, int sv[2]);//domain表示协议簇//type表示类型//protocol表示协议, SOCK_STREAM表示流协议(类似tcp), SOCK_DGRAM表示数据报协议(类似udp)//sv用于存储建立的套接字对, 也就是两个套接字文件描述符//成功返回0, 否则返回-1, 可以从errno获取错误信息 调用成功后sv[0], sv[1]分别存储一个文件描述符 向sv[0]中写入, 可以从sv[1]中读取 向sv[1]中写入, 可以从sv[0]中读取 进程调用socketpair后, fork子进程, 子进程会默认继承sv[0], sv[1]这两个文件描述符, 进而可以实现父子进程间通信。例如, 父进程向sv[0]中写入, 子进程从sv[1]中读取; 子进程向sv[1]中写入, 父进程从sv[0]中读取。 1.5 守护进程(daemon) 守护进程是一种特殊的后台进程, 它脱离于终端, 用于周期性的执行某种任务 进程组 每个进程都属于一个进程组 每个进程组都有一个进程组号, 也就是该组组长的进程号(PID) 一个进程只能为自己或者其子进程设置进程组号 会话 一个会话可以包含多个进程组, 这些进程组中最多只能有一个前台进程组(也可以没有), 其余为后台进程组 一个会话最多只能有一个控制终端 用户通过终端登录或者网络登录, 会创建一个新的会话 进程调用系统调用setsid可以创建一个新的会话, 调用setsid的进程不能是某个进程组的组长。setsid调用完成后, 该进程成为这个会话的首进程(领头进程), 同时变成一个新的进程组的组长, 如果该进程之前有控制终端, 该进程与终端的联系也被断开 创建守护进程的方式 fork子进程后, 父进程退出, 子进程执行setsid, 子进程即可成为守护进程。这种方式下, 子进程是会话的领头进程, 可以重新打开终端, 此时可以再次fork, fork产生的子进程无法再打开终端(只有会话的领头进程才能打开终端)。第二次fork并不是必须的, 只是为了防止子进程再次打开终端 linux提供了daemon函数(该函数并不是系统调用, 为库函数)用于创建守护进程 1.6 swoole tcp server示例1234567891011121314151617181920212223&lt;?php//创建server$serv = new Swoole\Server('0.0.0.0', 9501, SWOOLE_PROCESS, SWOOLE_SOCK_TCP);//设置server的参数$serv-&gt;set(array( 'reactor_num' =&gt; 2, //reactor thread num 'worker_num' =&gt; 3, //worker process num));//设置事件回调$serv-&gt;on('connect', function ($serv, $fd)&#123; echo "Client:Connect.\n";&#125;);$serv-&gt;on('receive', function ($serv, $fd, $reactor_id, $data) &#123; $serv-&gt;send($fd, 'Swoole: '.$data); $serv-&gt;close($fd);&#125;);$serv-&gt;on('close', function ($serv, $fd) &#123; echo "Client: Close.\n";&#125;);//启动server$serv-&gt;start(); 上述代码在cli模式下执行时, 经过词法分析, 语法分析生成opcode, 进而交由zend虚拟机执行 zend虚拟机在执行到$serv-&gt;start()时, 启动Swoole server 上述代码中设置的事件回调是在worker进程中执行, 后文会详细介绍Swoole server模型 二. swoole server2.1 base模式 说明 base模式采用多进程模型, 这种模型与nginx一致, 每个进程只有一个线程, 主进程负责管理工作进程, 工作进程负责监听端口, 接受连接, 处理请求以及关闭连接 多个进程同时监听端口, 会有惊群问题, linux 3.9之前版本的内核, Swoole没有解决惊群问题 linux 内核3.9及其后续版本提供了新的套接字参数SO_REUSEPORT, 该参数允许多个进程绑定到同一个端口, 内核在接受到新的连接请求时, 会唤醒其中一个进行处理, 内核层面也会做负载均衡, 可以解决上述的惊群问题, Swoole也已经加入了这个参数 base模式下, reactor_number参数并没有实际作用 如果worker进程数设置为1, 则不会fork出worker进程, 主进程直接处理请求, 这种模式适合调试 启动过程 php代码执行到$serv-&gt;start()时, 主进程进入int swServer_start(swServer *serv)函数, 该函数负责启动server 在函数swServer_start中会调用swReactorProcess_start, 这个函数会fork出多个worker进程 主进程和worker进程各自进入自己的事件循环, 处理各类事件 2.2 process模式 说明 这种模式为多进程多线程, 有主进程, manager进程, worker进程, task_worker进程 主进程下有多个线程, 主线程负责接受连接, 之后交给react线程处理请求。 react线程负责接收数据包, 并将数据转发给worker进程进行处理, 之后处理worker进程返回的数据 manager进程, 该进程为单线程, 主要负责管理worker进程, 类似于nginx中的主进程, 当worker进程异常退出时, manager进程负责重新fork出一个worker进程 worker进程, 该进程为单线程, 负责具体处理请求 task_worker进程, 用于处理比较耗时的任务, 默认不开启 worker进程与主进程中的react线程使用域套接字进行通信, worker进程之间不进行通信 启动过程 Swoole server启动入口: swServer_start函数, 123456789101112131415161718192021222324252627//php 代码中$serv-&gt;start(); 会调用函数, 进行server startint swServer_start(swServer *serv);// 该函数首先进行必要的参数检查static int swServer_start_check(swServer *serv);// 其中有,if (serv-&gt;worker_num &lt; serv-&gt;reactor_num)&#123; serv-&gt;reactor_num = serv-&gt;worker_num;&#125;//也就是说reactor_num &lt;= worker_num//之后执行factory start, 也就是swFactoryProcess_start函数//该函数会fork出manager进程, manager进程进而fork出worker进程以及task_worker进程if (factory-&gt;start(factory) &lt; 0)&#123; return SW_ERR;&#125;//然后主进程的主线程生成reactor线程if (serv-&gt;factory_mode == SW_MODE_BASE)&#123; ret = swReactorProcess_start(serv);&#125;else&#123; ret = swReactorThread_start(serv);&#125; 如果设置了daemon模式, 在必要的参数检查完后, 先将自己变为守护进程再fork manager进程, 进而创建reactor线程 主进程先fork出manager进程, manager进程负责fork出worker进程以及task_worker进程。worker进程之后进入int swWorker_loop(swServer *serv, int worker_id), 也就是进入自己的事件循环, task_worker也是一样, 进入自己的事件循环。 1234static int swFactoryProcess_start(swFactory *factory);//swFactoryProcess_start会调用swManager_start生成manager进程int swManager_start(swServer *serv);// manager进程会fork出worker进程以及task_worker进程 主进程pthread_create出react线程, 主线程和react线程各自进入自己的事件循环, reactor线程执行static int swReactorThread_loop(swThreadParam *param), 等待处理事件 12//主线程执行swReactorThread_start, 创建出reactor线程int swReactorThread_start(swServer *serv); 结构图Swoole process模式结构如下图所示, 上图并没有考虑task_worker进程, 在默认情况下, task_worker进程数为0 三. 请求处理流程(process模式)3.1 reactor线程与worker进程之间的通信 Swoole master进程与worker进程之间的通信如下图所示, Swoole使用SOCK_DGRAM, 而不是SOCK_STREAM, 这里是因为每个reactor线程负责处理多个请求, reactor接收到请求后会将信息转发给worker进程, 由worker进程负责处理,如果使用SOCK_STREAM, worker进程无法对tcp进行分包, 进而处理请求 swFactoryProcess_start函数中会根据worker进程数创建对应个数的套接字对, 用于reactor线程与worker进程通信(详见swPipeUnsock_create函数) 假设reactor线程有2个, worker进程有3个, 则reactor与worker之间的通信如下图所示, 每个reactor线程负责监听几个worker进程, 每个worker进程只有一个reactor线程监听(reactor_num &lt;= worker_num)。Swoole默认使用worker_process_id % reactor_num对worker进程进行分配, 交给对应的reactor线程进行监听 reactor线程收到某个worker进程的数据后会进行处理, 值得注意的是, 这个reactor线程可能并不是发送请求的那个reactor线程。 reactor线程与worker进程通信的数据包123456789101112131415161718192021222324252627//包头typedef struct _swDataHead&#123; int fd; uint32_t len; int16_t from_id; uint8_t type; uint8_t flags; uint16_t from_fd;#ifdef SW_BUFFER_RECV_TIME double time;#endif&#125; swDataHead;//reactor线程向worker进程发送的数据, 也就是worker进程收到的数据包typedef struct&#123; swDataHead info; char data[SW_IPC_BUFFER_SIZE];&#125; swEventData;//worker进程向reactor线程发送的数据, 也就是reactor线程收到的数据包typedef struct&#123; swDataHead info; char data[0];&#125; swPipeBuffer; 3.2 请求处理 master进程中的主线程负责监听端口(listen), 接受连接(accept, 产生一个fd), 接受连接后将请求分配给reactor线程, 默认通过fd % reactor_number进行分配, 之后通过epoll_ctl将fd加入到对应reactor线程中, 刚加入时监听写事件, 因为新接受连接创建的套接字写缓冲区为空, 故而一定可写, 会被立刻触发, 进而reactor线程进行一些初始化操作 存在多个线程同时操作一个epollfd(通过系统调用epoll_create创建)的情况 多个线程同时调用epoll_ctl是线程安全的(对应一个epollfd), 一个线程正在执行, 其他线程会被阻塞(因为需要同时操作epoll底层的红黑树) 多个线程同时调用epoll_wait也是线程安全的, 但是一个事件可能会被多个线程同时接收到, 实际中不建议多个线程同时epoll_wait一个epollfd。Swoole中也是不存在这种情况的, Swoole中每个reactor线程都有自己的epollfd 一个线程调用epoll_wait, 一个线程调用epoll_ctl, 根据man手册, 如果epoll_ctl新加入的fd已经准备好, 会使得执行epoll_wait的线程变成非阻塞状态(可以通过man epoll_wait查看相关内容)。1234//主线程, 如果fd已经ready, reactor线程会被唤醒epoll_ctl(epollfd, fd, ...);//reactor线程epoll_wait(epollfd,...) reactor线程中fd的写事件被触发, reactor线程负责处理, 发现是首次加入, 没有数据可写, 则会开启读事件监听, 准备接受客户端发送的数据 reactor线程读取到用户的请求数据, 一个请求的数据接收完后, 将数据转发给worker进程, 默认是通过fd % worker_number进行分配 reactor发送给worker进程的数据包, 会包含一个头部, 头部中记录了reactor的信息 如果发送的数据过大, 则需要将数据进行分片, 限于篇幅, 数据分片, 后续再进行详细讲述 可能存在多个reactor线程同时向同一个worker进程发送数据的情况, 故而Swoole采用SOCK_DGRAM模式与worker进程进行通信, 通过每个数据包的包头, worker进程可以区分出是由哪个reactor线程发送的数据, 也可以知道是哪个请求 worker进程收到reactor发送的数据包后, 进行处理, 处理完成后, 将请求结果发送给主进程 worker进程发送给主进程的数据包, 也会包含一个头部, 当reactor线程收到数据包后, 能够知道对应的reactor线程, 请求的fd等信息 主进程收到worker进程发送的数据包, 这个会触发某个reactor线程进行处理 这个reactor线程并不一定是之前发送请求给worker进程的那个reactor线程 主进程的每个reactor线程都负责监听worker进程发送的数据包, 每个worker发送的数据包只会由一个reactor线程进行监听, 故而只会触发一个reactor线程 reactor线程处理worker进程发送的请求处理结果, 如果是直接发送数据给客户端, 则可以直接发送, 如果需要改变这个这个连接的监听状态(例如close), 则需要先找到监听这个连接的reactor线程, 进而改变这个连接的监听状态(通过调用epoll_ctl) reactor处理线程与reactor监听线程可能并不是同一个线程 reactor监听线程负责监听客户端发送的数据, 进而转发给worker进程 reactor处理线程负责监听worker进程发送给主进程的数据, 进而将数据发送给客户端 四. gdb调试4.1 process模式启动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//fork manager进程#0 0x00007ffff67dae64 in fork () from /lib64/libc.so.6#1 0x00007ffff553888a in swoole_fork () at /root/code/swoole-src/src/core/base.c:186#2 0x00007ffff556afb8 in swManager_start (serv=serv@entry=0x1353f60) at /root/code/swoole-src/src/server/manager.cc:164#3 0x00007ffff5571dde in swFactoryProcess_start (factory=0x1353ff8) at /root/code/swoole-src/src/server/process.c:198#4 0x00007ffff556ef8b in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:651#5 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#6 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#7 execute_ex (ex=0x7ffff7f850a8) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#8 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#9 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#10 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#11 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#12 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389// pthread_create reactor线程#0 0x00007ffff552e960 in pthread_create@plt () from /usr/local/lib/php/extensions/no-debug-non-zts-20180731/swoole.so#1 0x00007ffff5576959 in swReactorThread_start (serv=0x1353f60) at /root/code/swoole-src/src/server/reactor_thread.c:883#2 0x00007ffff556f006 in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:670#3 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#4 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#5 execute_ex (ex=0x7fffffffab10) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#6 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#7 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#8 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#9 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#10 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389 4.2 base模式启动123456789101112131415161718192021222324252627282930313233//base 模式下的启动#0 0x00007ffff67dae64 in fork () from /lib64/libc.so.6#1 0x00007ffff553888a in swoole_fork () at /root/code/swoole-src/src/core/base.c:186#2 0x00007ffff5558557 in swProcessPool_spawn (pool=pool@entry=0x7ffff2d2a308, worker=0x7ffff2d2a778) at /root/code/swoole-src/src/network/process_pool.c:392#3 0x00007ffff5558710 in swProcessPool_start (pool=0x7ffff2d2a308) at /root/code/swoole-src/src/network/process_pool.c:227#4 0x00007ffff55741cf in swReactorProcess_start (serv=0x1353f60) at /root/code/swoole-src/src/server/reactor_process.cc:176#5 0x00007ffff556f21d in swServer_start (serv=0x1353f60) at /root/code/swoole-src/src/server/master.cc:666#6 0x00007ffff55dc808 in zim_swoole_server_start (execute_data=&lt;optimized out&gt;, return_value=0x7fffffffac50) at /root/code/swoole-src/swoole_server.cc:2946#7 0x00000000007bb068 in ZEND_DO_FCALL_SPEC_RETVAL_UNUSED_HANDLER () at /root/php-7.3.3/Zend/zend_vm_execute.h:980#8 execute_ex (ex=0x7ffff2d2a308) at /root/php-7.3.3/Zend/zend_vm_execute.h:55485#9 0x00000000007bbf58 in zend_execute (op_array=op_array@entry=0x7ffff5e7b340, return_value=return_value@entry=0x7ffff5e1d030) at /root/php-7.3.3/Zend/zend_vm_execute.h:60881#10 0x0000000000737554 in zend_execute_scripts (type=type@entry=8, retval=0x7ffff5e1d030, retval@entry=0x0, file_count=file_count@entry=3) at /root/php-7.3.3/Zend/zend.c:1568#11 0x00000000006db4d0 in php_execute_script (primary_file=primary_file@entry=0x7fffffffd050) at /root/php-7.3.3/main/main.c:2630#12 0x00000000007be2f5 in do_cli (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:997#13 0x000000000043fc1f in main (argc=2, argv=0x1165cd0) at /root/php-7.3.3/sapi/cli/php_cli.c:1389 五. 总结及思考 本文主要介绍了Swoole server的两种模式: base模式、process模式, 详细讲解了两种模式的网络编程模型, 并重点介绍了process模式下, 进程间通信的方式、请求的处理流程等。 process模式下, 为什么不直接在主进程中创建多个线程, 由线程直接进行处理请求(可以避免进程间通信的开销), 而是创建出manager进程, 再由manager进程创建出worker进程, 由worker进程处理请求? 个人觉得可能是php对多线程的支持不是很友好, phper大都也只是进行单线程编程 ZendVM 提供的 TSRM 虽然也是支持多线程环境，但实际上这是一个 按线程隔离内存的方案, 多线程并没有意义 process模式下, 主进程中的每个reactor线程都可以同时处理多个请求, 多个请求是并发处理的, 我们从2个维度看, 从主进程的角度看, 主进程同时处理多个请求, 当一个请求包全部接收完后, 转发给worker进程进行处理 从某个worker进程的角度看, 这个worker进程收到的请求是串行的, 默认情况下, worker进程也是串行处理请求, 如果单个请求阻塞(Swoole的worker进程会回调phper写的事件处理函数, 该函数可能阻塞), 后续的请求也无法处理, 这个就是排头阻塞问题, 这种情况下可以使用Swoole的协程, 通过协程的调度, 单个请求阻塞时, worker进程可以继续处理其他请求。 使用Swoole创建tcp server时, 由于tcp是字节流的协议, 需要分包, 而Swoole在不清楚客户端与服务端通信协议的情况下, 无法进行分包, process模式下, reactor交给worker进程的数据也只能是字节流的, 需要用户自行处理。当然, 一般情况也不需要自行构建协议, 使用tcp server, Swoole已经支持Http, Https等协议。 六. 参考 UNIX网络编程 UNIX环境高级编程 php7 底层实现与源码分析 https://wiki.swoole.com/ https://www.cnblogs.com/welhzh/p/3772164.html https://www.cnblogs.com/JohnABC/p/4079669.html]]></content>
      <categories>
        <category>swoole</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>server</tag>
        <tag>swoole</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置存储概述]]></title>
    <url>%2F2018%2F11%2F13%2Fnginx-config-store-summarize%2F</url>
    <content type="text"><![CDATA[一. 基础nginx的一般配置如下所示:123456789101112131415161718192021222324252627282930...work_porcess: xx;events&#123; ... work_connections xx;&#125;http&#123; //第一级别的配置块 ... upstream xx&#123; //第二级别的配置块 ... &#125; server&#123; //第二级别的配置块 ... location /&#123; //第三级别的配置块 .... location /&#123; //第四级别的配置块 ... &#125; &#125; &#125; server&#123; //第二级别的配置块 ... &#125;&#125; 如何存储上述结构? 由于配置中存在嵌套, 可以使用树型结构进行存储 如何使用上述配置? 从上到下, 按层查找 配置解析核心? 对于块中嵌套块的模块存储(例如http), 核心在于要以块为单位进行分析。例如, 最外层为http块, 属于一级配置; http中的server或者upstream属于二级配置块; server中的location属于三级配置块; 依次类推, 每块都看作一个完整的结构。 由于存在块中嵌套, 内层的块有些配置需要从外层块继承, 有些配置不能继承, 故而每块的配置需要按区域划分。 二级配置块需要放到一级配置块中, 这样才能从一级配置块中找到所有的二级配置块, 以此类推。 由于配置解析是依靠各个模块完成的, 故而配置存储的树形结构中需要以模块为单位存储(也就是每个模块具有自己的存储结构, 可以放在树形结构的某个位置上)。 二. nginx配置存储结构由于结构比较复杂, 此处分为2部分。 第一部分配置存储结构图 图中只给出了http中的server配置, 没有画出server中的location以及location嵌套的location 第二部分配置存储结构图 三. http块存储结构对于http的配置存储, 应当以块为单位进行分析, 总体而言, http块按树形存储 大体上可以分为3个级别(不考虑location中的location), 每个级别的配置分为3块(main块, srv块, loc块) 第二级别的配置继承第一级别的main块配置 第三级别的配置继承第二级别的main块以及srv块配置 第四及其后续级别的配置继承前一级的main块, srv块配置 四. nginx upstream块存储 upstream的配置与server类似, 属于第二级别配置块 upstream的配置块中main块从第一级别的main块继承 五. nginx proxy存储 proxy_pass出现在location块中, 属于第三或者之后级别的配置。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>配置存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx listen指令处理流程]]></title>
    <url>%2F2018%2F11%2F12%2Fnginx-listen-socket%2F</url>
    <content type="text"><![CDATA[一. 基础 nginx源码采用1.15.5 后续部分仅讨论http中的listen配置解析以及优化流程 1.1 概述 假设nginx http模块的配置如下 12345678910111213141516171819202122232425262728293031323334http&#123; server &#123; listen 127.0.0.1:8000; server_name www.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 10.0.1.1:8000; server_name www.news.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 8000; #相当于0.0.0.0:8000 server_name www.tieba.baidu.com; root html; location /&#123; index index.html; &#125; &#125; server &#123; listen 127.0.0.1:8000; server_name www.zhidao.baidu.com; location / &#123; root html; index index.html; &#125; &#125;&#125; 端口, 地址, server的关系 端口是指一个端口号, 例如上面的8000端口 地址是ip+port, 例如127.0.0.1:8000, 10.0.1.1:8000, 0.0.0.0:8000, listen后配置的是一个地址。 每个地址可以放到多个server中, 例如上面的127.0.0.1:8000 总而言之, 一个端口可以有多个地址, 每个地址可以有多个server 1.2 存在的问题 是否需要在读取完http块中所有的server才能建立监听套接字, 绑定监听地址? 是的, 因为允许配置通配地址, 故而必须将http块中的server全部读取完后, 才能知道如何建立监听套接字。 一个端口可以对应多个地址, 如何建立监听套接字, 如何绑定地址? 通常情况下, 每个地址只能绑定一次(只考虑tcp协议), 这种情况下, 我们只能选择部分地址创建监听套接字, 绑定监听地址。 当配置中存在通配地址(0.0.0.0:port)时, 只需要创建一个监听套接字, 绑定这个通配地址即可, 但需要能够依据该监听套接字找到该端口配置的其他地址, 这样当客户端发送请求时, 可以根据客户端请求的地址, 找到对应地址下的相关配置。 当配置中不存在通配地址时, 需要对每个地址都创建一个监听套接字, 绑定监听地址。 一个地址多个server的情况下, 如何快速找到客户端请求的server? 比较合适的方案是通过hash表。 为了快速找到客户端请求的server, nginx以server_name为key, 每个server块的配置(可以理解为一个指针, 该指针指向整个server块的配置)为value, 放入到哈希表。 由于server_name中可以出现正则匹配等情况, nginx将server_name具体分为4类进行分别处理(www.baidu.com, *baidu.com, www.baidu\*, ~*baidu)。 1.3 nginx listen解析的流程总体而言分为2步， 将所有http模块内的配置解析完成, 将listen的相关配置暂存(主要存储监听端口以及监听地址)。 根据上一步暂存的监听端口以及监听地址, 创建监听套接字, 绑定监听地址 二. 配置解析nginx http块解析完成后, 会存储配置文件中配置的监听端口以及监听地址, 其核心结构图如下, 总体而言, 结构可以分为3级, 端口-&gt;地址-&gt;server 2.1 源码listen的处理流程: ngx_http_core_listen: 读取配置文件配置 ngx_http_add_listen: 查看之前是否出现过当前监听的端口, 没有则新建, 否则追加 ngx_http_add_address: 查看之前该端口下是否监听过该地址, 没有则新建, 否则追加。 ngx_http_add_server: 查看server之前是否出现过, 没有则新建, 否则报错(重复定义)。 三. 创建监听套接字nginx最终创建的监听套接字及其相关的结构图如下, 每个ngx_listening_t结构对应一个监听套接字, 绑定一个监听地址 每个ngx_listening_t结构后面需要存储地址信息, 地址可能不止一个, 因为这个监听套接字可能绑定的是通配地址, 这个端口下的其他地址都会放在这个监听套接字下。例如, 1.1节的配置中, 只会创建一个ngx_listening_t结构, 其他地址的配置都会放到这个通配地址下。 每个监听地址可能对应多个域名(配置文件中的server_name), 需要将这些域名放到哈希表中, 以供后续使用 总体而言, 结构分为3级, 监听套接字-&gt;监听地址-&gt;server 3.1 源码读取完http块后, 需要创建监听套接字绑定监听地址, 处理函数ngx_http_optimize_servers, 该函数的处理流程: 遍历所有监听端口, 针对每个监听端口, 执行以下3步 对该端口下所有监听地址排序(listen后配置bind的放在前面, 通配地址放在后面) 遍历该端口下的所有地址, 将每个地址配置的所有server, 放到该地址的哈希表中。 为该端口建立监听套接字, 绑定监听地址。 四. 监听套接字的使用 假设此处我们使用epoll作为事件处理模块 epoll在增加事件时, 用户可以使用epoll_event中的data字段, 当事件发生时, 该字段也会带回。 nginx中的epoll_event指向的是ngx_connection_t结构, 事件发生时, 调用ngx_connection_t结构中的读写事件, 负责具体处理事件, 参见下图。12345//c is ngx_connection_trev = c-&gt;read;rev-&gt;hadler(rev);wev = c-&gt;write;wev-&gt;handler(wev); 每个监听套接字对应一个ngx_connection_t, 该结构的读事件回调函数为ngx_event_accept, 当用户发起tcp握手时, 通过ngx_event_accept接受客户端的连接请求。 ngx_event_accept会接受客户端请求, 初始化一个新的ngx_connection_t结构, 并将其加入到epoll中进行监听, 最后会调用ngx_connection_t对应的ngx_listening_t的处理函数(http块对应ngx_http_init_connection, mail块ngx_mail_init_connection, stream块对应ngx_stream_init_connection) 五. 总结 nginx在读取listen相关的配置时, 将结构分为3级, 端口-&gt;地址-&gt;server, 各级都是一对多的关系。 nginx在创建监听套接字时, 将结构分为3级, 监听套接字-&gt;地址-&gt;server, 各级都是一对多的关系。]]></content>
      <categories>
        <category>nginx</category>
        <category>配置解析</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>网络编程</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向代理]]></title>
    <url>%2F2018%2F11%2F09%2Fbrowser-proxy%2F</url>
    <content type="text"><![CDATA[一. 概述 什么是正向代理? 简单来说就是代理客户端请求的服务器。例如, 浏览器中设置代理翻墙等。 正向代理的主要问题? 代理服务器需要知道客户的目标服务器, 例如: 客户一会请求www.baidu.com, 一会请求www.taobao.com, 代理服务器如何获取目标服务器信息。 正向代理问题的解决方案? 客户端按照某种事先约定的协议通知代理服务器, 例如sock5协议 代理服务器能够直接从客户端与服务端交互的上层数据包(tcp之上)获取目标服务器信息。例如: 客户端与目标服务器按照http协议通信, 代理服务器可以直接从数据包中解析目标服务器。这种情况不适用与客户端与目标服务器加密通信的情况, 例如https。 二. socks52.1 socks5基本流程sock5通信的基本流程图如下, 如果不需要认证, 第三步和第四步则不需要。 2.2 socks5总结 socks5主要完成目标地址传递的功能 浏览器与sock5服务器完成tcp握手 浏览器将目标地址通过socks5协议发送给socks5服务器 socks5服务器与目标地址完成tcp握手 浏览器将请求数据发送给socks5服务器, 由其进行转发, 此时可以只做4层tcp代理 三. shadowsocks原理shadowsocks信息交换的基本结构图如下, ss_cli代表shadowsocks客户端, ss_srv代表shadowsocks服务端, 3.1 第一阶段 客户端与ss_cli建立tcp连接 客户端与ss_cli协商sock认证参数, ss_cli给与响应, 默认不进行认证 客户端将目标地址发送给ss_cli, ss_cli给与响应, 默认返回成功 3.2 第二阶段 ss_cli与ss_srv建立tcp连接 ss_cli将客户端发送的目标地址发送给ss_srv, 此时按照ss_cli与ss_srv协商好的数据包格式, 并不需要使用sock协议, 同时还会发送加密解密需要的随机数 ss_srv与目标地址建立tcp连接 3.3 第三阶段 客户端向ss_cli发送tcp数据包 ss_cli将数据包稍作处理, 计算长度, 计算哈希值, 并将这些信息一并发送给ss_srv, 此时的数据包格式是ss_cli与ss_srv约定好的 ss_srv受到数据后, 解析后发送给目标服务器 3.4 第四阶段 ss_srv收到目标服务器返回的数据后, 将数据按约定好的格式转发给ss_cli ss_cli受到数据后, 解析转发给客户 3.5 总结 通过sock5协议, ss_cli可以知道客户端的目标服务器地址 ss_cli与ss_srv可以自行定义协议, 核心是需要将客户端的目标服务器地址以及解密需要的随机变量发送给ss_srv ss_cli, ss_srv不需要也无法知道客户端与目标服务器的具体通信信息。 ss_cli, ss_srv后期是tcp的代理, 不管是https还是http对其而言都是一样的。 3.6 参考 https://www.jianshu.com/p/cbea16a096fb 四. 抓包分析 ss_cli: 127.0.0.1:1080 ss_srv: 204.48.26.173:21500 浏览器与ss_cli同属于一个主机, 通信时延低。ss_cli与ss_srv需要通过网络传输, 时延高。数据包分析中可以利用这一点。 4.1 浏览器与ss_cli通信 tcp握手, 浏览器通过socks5发送目标服务器地址 浏览器与ss_cli之间后续的通信, 此处是https协议 时间突变的部分是由于ss_cli需要ss_srv将目标服务器的应答信息发送回来。 4.2 ss_cli与ss_srv通信二者的通信格式是自行定义的,]]></content>
      <categories>
        <category>网络编程</category>
        <category>代理</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结构体传输]]></title>
    <url>%2F2018%2F11%2F05%2Fstruct-transmission%2F</url>
    <content type="text"><![CDATA[一. 基础 结构体传输基本上有两种方式,序列化(Json,Xml等)以及直接传输结构体。 下面考虑32位系统，直接发送结构体进行传输。 二. 结构体12345struct Data&#123; char v1; int v2; char v3;&#125; 三. 源码3.1 发送方123456789101112131415161718//将data写入bufbool send(char *buf, int bufLen, const Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memcpy(buf, data, sizeof(*data); return true;&#125;//main functionint main(int argc, char **argv)&#123; char sendBuf[256] = &#123;&#125;; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; $ans = send(sendBuf, sizeof(sendBuf), &amp;data); if($ans == false) return -1; //send data&#125; 3.2 接收方123456789101112131415161718//deal databool parseData(char *buf, int bufLen, Data *data)&#123; if(bufLen &lt; sizeof(*data)) return false; memecpy(buf, data, sizeof(*data)); data.v2 = ntohl(data.v2); return true;&#125;//mainint main(int argc, char **argv)&#123; char recvBuf[256] = &#123;&#125;; //read socket recv data //deal data Data data; $ans = parseData(buf, sizeof(recvBuf), &amp;data); if($ans == false) return -1; //Deal data&#125; 3.3 测试123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;arpa/inet.h&gt;using namespace std;struct Data&#123; char v1; int v2; char v3;&#125;;void parse(char *buf, Data *data)&#123; memcpy(data, buf, sizeof(*data)); data-&gt;v2 = ntohl(data-&gt;v2);&#125;int main (int argc, char **argv)&#123; Data data; data.v1 = 'a'; data.v2 = htonl(2); data.v3 = 'b'; char buf[128] = &#123;&#125;; memcpy(buf, &amp;data, sizeof(data)); Data newData; parse(buf, &amp;newData); std::cout &lt;&lt; newData.v1 &lt;&lt; ' ' &lt;&lt; newData.v2 &lt;&lt; ' ' &lt;&lt; newData.v3 &lt;&lt; std::endl; return 0;&#125;//输出: a, 2, b 四. 注意事项 sizeof(data) = 12; 结构体要考虑对齐,发送方与接收方的对齐方式应该是一致的。 发送方按照网络字节序存储,接收方得到网络字节序的数据后，解析成本机字节序。]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节序与位序]]></title>
    <url>%2F2018%2F11%2F05%2Fnet-byte-order%2F</url>
    <content type="text"><![CDATA[一. 本机字节序 小端: 低位字节存在低地址。 大端: 低位字节存在高地址。 二. 本机位序一般情况下，本机位序与本机的字节序一致。 小端字节序: 低位bit存在低地址。 大端字节序: 低位bit存在高地址。 三. 网络序 网络字节序(大端)，先传送高位字节，再传送低位字节。 在传输一个字节时，先传送低位bit, 再传送高位bit。 注: 指针指向变量或者数组的起始地址，即指向低地址。 四. 代码示例12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main (int argc, char **argv)&#123; union byte_order&#123; int a; unsigned char b[4]; &#125;; byte_order val; val.a = 0x01020304; printf("address 0x%x byte: 0x%x\n", &amp;val.b[0], val.b[0]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[1], val.b[1]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[2], val.b[2]); printf("address 0x%x byte: 0x%x\n", &amp;val.b[3], val.b[3]); struct bit_order&#123; unsigned char a:4; unsigned char b:4; &#125;; unsigned char tmp = 0x04; bit_order *val1 = (bit_order*)&amp;tmp; printf("low bit %d\n", val1-&gt;a); printf("high bit %d\n", val1-&gt;b);&#125;//输出:address 0x56074a68 byte: 0x4address 0x56074a69 byte: 0x3address 0x56074a6a byte: 0x2address 0x56074a6b byte: 0x1low bit 4high bit 0]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>数据传输顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[水平触发与边沿触发]]></title>
    <url>%2F2018%2F11%2F05%2FLT-ET%2F</url>
    <content type="text"><![CDATA[一. 基础1.1 水平触发 基本概念 读缓冲区不为空时, 读事件触发。 写缓冲区不为满时, 写事件触发。 处理流程 accept新的连接, 监听读事件。 读事件到达, 处理读事件。 需要写入数据, 向fd中写数据, 一次无法写完, 开启写事件监听。 写事件到达, 继续写入数据, 写完后关闭写事件。 优缺点 不会遗漏事件, 易编程。 长连接需要写入的数据量大时, 会频繁开启关闭写事件。 1.2 边沿触发 基本概念 读缓冲区状态变化时, 读事件触发, 网卡接受到新数据。 写缓冲区状态变化时, 写事件触发, 网卡发出了新数据。 处理流程 accept新的连接, 同时监听读写事件。 读事件到达, 需要一直读取数据, 直到返回EAGAIN。 写事件到达, 无数据处理则不处理, 有数据待写入则一直写入，直到写完或者返回EAGAIN。 优缺点 不需要频繁开启关闭事件, 效率较高。 读写事件处理不当, 可能导致事件丢失, 编程教复杂。 1.3 选择 概述 对于读事件而言，总体而言, 采用水平触发方式较好。应用程序在读取数据时，可能会一次无法读取全部数据，边沿触发在下一次可能不会触发。如果能够保证一次读取缓存的全部数据，可以采用边沿触发，效率更高, 但同时编程复杂度也高。 对于写事件，当客户端服务端采用短连接或者采用长连接但发送的数据量比较少时(例如: Redis), 采用水平触发即可。当客户端与服务端是长连接并且数据写入的量比较大时(例如: nginx), 采用边沿触发, 因为边沿触发效率更高。 目前，linux不支持读写事件分别设置不同的触发方式，具体采用哪种方式触发，需要根据具体需求。 监听套接字事件设置 监听套接字不需要监听写事件，只需要监听读事件。 监听套接字一般采用水平触发方式。(nginx开启multi_accept时，会把监听套接字所有可读的事件全部读取，此时可以使用边沿触发。但为了保证连接不丢失，nginx仍然采用水平触发) 通信套接字设置 redis对于与客户端通信使用的套接字默认使用水平触发。 nginx对于与客户端通信使用的套接字默认采用边沿触发。 二. 参考 https://blog.csdn.net/dongfuye/article/details/50880251]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F11%2F01%2Fload_balance%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 基础 什么是负载均衡? 当单机提供的并发量不能满足需求时，我们需要多台服务器同时服务。当客户请求到达时，如何为客户选择最合适的服务器?这个问题就是负载均衡问题。 负载均衡主要需要解决的问题是哪些? 从客户端的角度上看，客户需要最快速的得到服务器的相应，负载均衡时需要找出能最快相应客户需求的服务器进行服务。 从服务端来看如何使得每台服务器都能达到较高的利用率，最大限制的为用户提供快速、可靠的服务是服务端需要考虑的主要问题。 1.2 负载均衡分类 硬件 F5 软件 dns负载均衡 LVS负载均衡(4层) nginx, haproxy(7层) 二. F5负载均衡 F5是一家美国的公司，该公司生产一些硬件设备可以作为负载均衡器使用(例如:big-ip), 本文后续部分所说的F5是指其负载均衡器产品。 不同的产品实现的功能不一致，具体情况需要根据产品说明书。 F5可以在4-7层内做负载均衡，用户可以根据需求进行配置。 由于F5可以做7层负载均衡，故而可以实现会话管理，http处理等。 2.1 数据转发模式 standard类型, 这种模式下，客户端与F5服务器建立连接，F5服务器与真实服务器建立连接，F5服务器将客户需求转发给真实服务器，并将真实服务器的相应转发给客户端，此时F5可以查看请求和相应的所有信息。 四层转发模式(performance L4), 这种模式下，F5只处理4层以下的数据。客户端将数据发送给F5, F5仅将数据转发给真实服务器，包括TCP的握手数据包以及挥手数据包，真实服务器需要先将数据发送给F5服务器，F5将其转发给客户端。 路由模式, 这种模式与LVS的DR模式类似。 … 2.2 负载均衡算法 轮询，加权轮询。 源地址哈希 … 2.3 小结F5的优势在于功能强大，并发量高，能满足客户的大多数需求，但其成本较高，一般大型国企可能会使用。 2.4 参考 https://f5.com/zh https://www.jianshu.com/p/2b55aa4c21e2 https://wenku.baidu.com/view/450b8643cc7931b765ce15c1.html 三. dns负载均衡 dns负载均衡由dns服务提供厂商提供。 最初的dns负载均衡提供简单轮询，不能根据客户端或者服务端状态进行选择。 目前，有些dns服务厂商可以提供智能dns服务，用户可以设置负载均衡方案，例如：根据客户端ip地址，选择就近的服务器。 对于目前大多数的公司而言，为了更好的服务用户，通常会使用dns负载均衡，将用户按照就近原则，分配到某个集群服务器上。之后，集群内再采用其他的负载均衡方案。 四. Linux Virtual Server(LVS) LVS通过修改数据包Ip地址，Mac地址实现负载均衡。 LVS由ipvs(内核中), ipvsadm(用户态)组成。LVS需要理解tcp，ip头部。 当tcp握手信号，SYN数据包达到时，ipvs选择一个后端服务器，将数据包进行转发。在此之后，所有包含相同的ip，tcp头部的数据包都会被转发到之前选择的服务器上。很明显，ipvs无法感知数据包内容。 4.1 分类 LVS-NAT LVS-DR LVS-TUN 4.2 基本原理4.2.1 LVS-DRLVS-DR模式的基本原理如下图所示: 4.2.2 LVS-NATLVS-NAT模式的基本原理如下图所示: 4.3 负载均衡算法4.3.1 静态算法 轮询(Round Robin, RR) 加权轮询(Weight Round Robin, WRR) 源地址Hash(Source Hash, SH) 目的地址Hash(Destination Hash, DH), 可以设置多个VIP 4.3.2 动态算法 最少连接(Least Connections, LC)，找出当前连接数最小的服务器 加权最少连接(Weighted Least Connections, WLC) 最短期望延迟(Shortest Expected Delay Scheduling, SED) 基于WLC。例如: 现有A, B, C三台服务器，权重分别为100,200,300，当前的连接数分别为1,2,3,下一个连接到达时，通过计算期望时延选择服务器(1+1)/100, (2+1)/200, (3+1)/300, 故而选择C服务器。 永不排队(Never Queue Scheduling, NQ)， 改进的sed, 如果某台服务器连接数为0，直接连接过去，不在进行sed计算。 基于局部性的最少连接(locality-Based Least Connections, LBLC)，根据目标ip, 找出目标ip最近使用的服务器，如果服务器存在并且负载没有大于一个阈值，则将新的连接分配到这个服务器上，否则按照最少连接找出一个服务器处理该请求。 带复制的基于局部性最少连接(Locality-Based Least Connections with Replication, LBLCR)，根据目标ip，维护一个服务器组，每次从组中挑选服务器，如果服务器不可以处理，则从所有服务器中按照最少连接挑选出一台服务器，并将其加入到目标ip的处理组服务器中。 4.3 参考 https://liangshuang.name/2017/11/19/lvs/ 五. Nginx Load Balance nginx负载均衡工作在7层，它会与client、upstream分别建立tcp连接，nginx需要维护这两个连接的状态。 nginx的stream模块可以用于4层负载均衡，但一般很少使用。 5.1 基本原理nginx做7层负载均衡的基本原理如下图所示: 5.2 负载均衡算法 轮询(默认) 加权轮询 源ip哈希 响应时间 url 哈希]]></content>
      <categories>
        <category>架构</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>分布式技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx内存管理]]></title>
    <url>%2F2018%2F10%2F31%2Fnginx_memory_manage%2F</url>
    <content type="text"><![CDATA[一. 概述 应用程序的内存可以简单分为堆内存，栈内存。对于栈内存而言，在函数编译时，编译器会插入移动栈当前指针位置的代码，实现栈空间的自管理。而对于堆内存，通常需要程序员进行管理。我们通常说的内存管理亦是只堆空间内存管理。 对于内存，我们的使用可以简化为3步，申请内存、使用内存、释放内存。申请内存，使用内存通常需要程序员显示操作，释放内存却并不一定需要程序员显示操作，目前很多的高级语言提供了垃圾回收机制，可以自行选择时机释放内存，例如: Go、Java已经实现垃圾回收, C语言目前尚未实现垃圾回收，C++中可以通过智能指针达到垃圾回收的目的。 除了语言层面的内存管理外，有时我们需要在程序中自行管理内存，总体而言，对于内存管理，我认为主要是解决以下问题: 用户申请内存时，如何快速查找到满足用户需求的内存块？ 用户释放内存时，如何避免内存碎片化？ 无论是语言层面实现的内存管理还是应用程序自行实现的内存管理，大都将内存按照大小分为几种，每种采用不同的管理模式。常见的分类是按照2的整数次幂分，将不同种类的内存通过链表链接，查询时，从相应大小的链表中寻找，如果找不到，则可以考虑从更大块内存中，拿取一块，将其分为多个小点的内存。当然，对于特别大的内存，语言层面的内存管理可以直接调用内存管理相关的系统调用，应用层面的内存管理则可以直接使用语言层面的内存管理。 nginx内存管理整体可以分为2个部分， 第一部分是常规的内存池，用于进程平时所需的内存管理； 第二部分是共享内存的管理。总体而言，共享内存教内存池要复杂的多。 二. nginx内存池管理2.1 说明 本部分使用的nginx版本为1.15.3 具体源码参见src/core/ngx_palloc.c文件 2.2 nginx实现2.2.1 使用流程nginx内存池的使用较为简单,可以分为3步， 调用ngx_create_pool函数获取ngx_pool_t指针。 12//size代表ngx_pool_t一块的大小ngx_pool_t* ngx_create_pool(size_t size, ngx_log_t *log) 调用ngx_palloc申请内存使用 12//从pool中申请size大小的内存void* ngx_palloc(ngx_pool_t *pool, size_t size) 释放内存(可以释放大块内存或者释放整个内存池) 1234//释放从pool中申请的大块内存ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p)//释放整个内存池void ngx_destroy_pool(ngx_pool_t *pool) 2.2.2 具体实现 如下图所示，nginx将内存分为2种，一种是小内存，一种是大内存，当申请的空间大于pool-&gt;max时，我们认为是大内存空间，否则是小内存空间。12//创建内存池的参数size减去头部管理结构ngx_pool_t的大小pool-&gt;max = size - sizeof(ngx_pool_t); 对于小块内存空间, nginx首先查看当前内存块待分配的空间中，是否能够满足用户需求，如果可以，则直接将这部分内存返回。如果不能满足用户需求，则需要重新申请一个内存块，申请的内存块与当前块空间大小相同，将新申请的内存块通过链表链接到上一个内存块，从新的内存块中分配用户所需的内存。 小块内存并不释放，用户申请后直接使用，即使后期不再使用也不需要释放该内存。由于用户有时并不知道自己使用的内存块是大是小，此时也可以调用ngx_pfree函数释放该空间，该函数会从大空间链表中查找内存，找到则释放内存。对于小内存而言，并未做任何处理。 对于大块内存, nginx会将这些内存放到链表中存储，通过pool-&gt;large进行管理。值得注意的是，用户管理大内存的ngx_pool_large_t结构是从本内存池的小块内存中申请而来，也就意味着无法释放这些内存，nginx则是直接复用ngx_pool_large_t结构体。当用户需要申请大内存空间时，利用c函数库malloc申请空间，然后将其挂载某个ngx_pool_large_t结构体上。nginx在需要一个新的ngx_pool_large_t结构时，会首先pool-&gt;large链表的前3个元素中，查看是否有可用的,如果有则直接使用，否则新建ngx_pool_large_t结构。 三. nginx共享内存管理3.1 说明 本部分使用的nginx版本是1.15.3 本部分源码详见src/core/ngx_slab.c, src/core/ngx_shmtx.c nginx共享内存内容相对较多，本文仅做简单概述。 3.2 直接使用共享内存3.2.1 基础 nginx中需要创建互斥锁，用于后面多进程同步使用。除此之外，nginx可能需要一些统计信息，例如设置(stat_stub),对于这些变量，我们并不需要特意管理，只需要开辟共享空间后，直接使用即可。 设置stat_stub后所需的统计信息，亦是放到共享内存中，我们此处仅以nginx中的互斥锁进行说明。 3.2.2 nginx互斥锁的实现 nginx互斥锁，有两种方案，当系统支持原子操作时，采用原子操作，不支持时采用文件锁。本节源码见ngx_event_module_init函数。 下图为文件锁实现互斥锁的示意图。 下图为原子操作实现互斥锁的示意图。 问题 reload时，新启动的master向老的master发送信号后直接退出，旧的master,重新加载配置(ngx_init_cycle函数), 新创建工作进程, 新的工作进程与旧的工作进程使用的锁是相同的。 平滑升级时, 旧的master会创建新的master, 新的master会继承旧的master监听的端口(通过环境变量传递监听套接字对应的fd)，新的进程并没有重新绑定监听端口。可能存在新老worker同时监听某个端口的情况，此时操作系统会保证只会有一个进程处理该事件(虽然epoll_wait都会被唤醒)。 3.3 通过slab管理共享内存 nginx允许各个模块开辟共享空间以供使用,例如ngx_http_limit_conn_module模块。 nginx共享内存管理的基本思想有: 将内存按照页进行分配，每页的大小相同, 此处设为page_size。 将内存块按照2的整数次幂进行划分, 最小为8bit, 最大为page_size/2。例如，假设每页大小为4Kb, 则将内存分为8, 16, 32, 64, 128, 256, 512, 1024, 2048共9种，每种对应一个slot, 此时slots数组的大小n即为9。申请小块内存(申请内存大小size &lt;= page_size/2)时，直接给用户这9种中的一种，例如，需要30bit时，找大小为32的内存块提供给用户。 每个页只会划分一种类型的内存块。例如，某次申请内存时，现有内存无法满足要求，此时会使用一个新的页，则这个新页此后只会分配这种大小的内存。 通过双向链表将所有空闲的页连接。图中ngx_slab_pool_t中的free变量即使用来链接空闲页的。 通过slots数组将所有小块内存所使用的页链接起来。 对于大于等于页面大小的空间请求，计算所需页数，找到连续的空闲页，将空闲页的首页地址返回给客户使用，通过每页的管理结构ngx_slab_page_t进行标识。 所有页面只会有3中状态，空闲、未满、已满。空闲，未满都是通过双向链表进行整合，已满页面则不存在与任何页面，当空间被释放时，会将其加入到某个链表。 nginx共享内存的基本结构图如下: 在上图中，除了最右侧的ngx_slab_pool_t接口开始的一段内存位于共享内存区外，其他内存都不是共享内存。 共享内存最终是从page中分配而来。]]></content>
      <categories>
        <category>nginx</category>
        <category>内存管理</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
